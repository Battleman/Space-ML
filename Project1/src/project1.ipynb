{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from preprocessing import preprocessing\n",
    "from features_engineering import augment\n",
    "from ml_methods import *\n",
    "from proj1_helpers import *\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = '../data/train.csv' \n",
    "DATA_TEST_PATH = '../data/test.csv'\n",
    "y, tX_train, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding a second axis to y for dimension compatitbility reasons\n",
    "y=y[:,np.newaxis]\n",
    "\n",
    "#computing the size of the prediction to generate\n",
    "pred_len=tX_test.shape[0]\n",
    "\n",
    "#all available regression functions\n",
    "lq=lambda a,b,c,d,e,f:least_squares(a,b)\n",
    "lqgd=lambda a,b,c,d,e,f: least_squares_GD(a, b, d, e, f)\n",
    "lqsgd=lambda a,b,c,d,e,f: least_squares_SGD(a, b, d, e, f)\n",
    "r=lambda a,b,c,d,e,f: ridge_regression(a, b, c)\n",
    "lgd=lambda a,b,c,d,e,f: logistic_regression_GD(a,b,d,e,f)\n",
    "lsgd=lambda a,b,c,d,e,f: logistic_regression_SGD(a,b,d,e,f)\n",
    "rlgd=reg_logistic_regression_GD\n",
    "rlsgd=reg_logistic_regression_SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(perc,y,x):\n",
    "    \"\"\"computes a (perc, 1-perc) split of x and y\"\"\"\n",
    "    np.random.seed(seed=1)\n",
    "    sample_size=len(y)\n",
    "    cut_ind=int(perc*sample_size)\n",
    "    shuffle_indices = np.random.permutation(sample_size)\n",
    "    shuffled_x=x[shuffle_indices]\n",
    "    shuffled_y=y[shuffle_indices]\n",
    "    return shuffled_y[:cut_ind], shuffled_x[:cut_ind],shuffled_y[cut_ind:], shuffled_x[cut_ind:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_expand(xtr,xte,degrees):\n",
    "    \"\"\"preprocesses and then expands the samples\"\"\"\n",
    "    (xstr, mtr) = preprocessing(xtr)\n",
    "    (xste, mte) = preprocessing(xte)\n",
    "    for it,(degree,xtr,xte) in enumerate(zip(degrees,xstr,xste)):\n",
    "        xstr[it]=augment(xtr,degree)\n",
    "        xste[it]=augment(xte,degree)\n",
    "    return xstr, mtr, xste, mte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_labels(tx,w,logistic):\n",
    "    \"\"\"generates class predictions given weights, and a test data matrix\"\"\"\n",
    "    y_pred= 2*sigmoid(tx.dot(w))-1 if logistic else tx.dot(w)\n",
    "    y_pred[np.where(y_pred <= 0)] = -1\n",
    "    y_pred[np.where(y_pred > 0)] = 1\n",
    "    return y_pred\n",
    "\n",
    "def predictions(ys, tx, px, mask_t, mask_p, reg_fs, len_pred, lambdas, max_iters, gammas, logistics):\n",
    "    \"\"\"generates predictions using the regression function reg_f (trained on y,tx) and the inputs px\"\"\"\n",
    "    y_pred = np.zeros((len_pred,1))\n",
    "    for x_train, mask_train, x_test, mask_test, lambda_, max_iter, gamma, logistic, reg_f, y_i in zip(tx, mask_t, px, mask_p, lambdas, max_iters, gammas, logistics, reg_fs, ys):\n",
    "        #print(\"#######New subset#######\")\n",
    "        y_correspond = y_i[mask_train]\n",
    "        x_train\n",
    "        #print(\"Augmented train\")\n",
    "        initial_w= np.zeros((x_train.shape[1], 1))\n",
    "        w,_ = reg_f(y_correspond, x_train, lambda_, initial_w, max_iter, gamma)\n",
    "        #print(\"Computed weights\")\n",
    "        x_test= x_test\n",
    "        #print(\"Augmented test\")\n",
    "        y_pred[mask_test] = predict_labels(x_test,w,logistic)\n",
    "        #print(\"Computed predictions\")\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_cross(ytr, yte, xstr, mtr, xste, mte, reg_fs, lambdas, max_iters, gammas, logistics):\n",
    "    \"\"\"estimates the true performance of reg_fs through basic cross-validation between \n",
    "    a test and training set of samples\"\"\"\n",
    "    ytr_l=np.where(ytr==-1,0,1)\n",
    "    size_pred=len(yte)\n",
    "    ys=[(ytr_l if l else ytr) for l in logistics]\n",
    "    y_bar=predictions(ys, xstr, xste, mtr, mte, reg_fs, size_pred, lambdas, max_iters, gammas, logistics)\n",
    "    return np.sum((yte==y_bar))/len(yte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split percentage for the cross-validation\n",
    "perc=0.8\n",
    "#generating the training and test sets\n",
    "ytr,xtr,yte,xte=split(perc,y,tX_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#degrees of expansion for each of the subsets generated by the preprocessing\n",
    "degrees=[3,3,3]\n",
    "\n",
    "#preprocessing and expanding both the training and the test set\n",
    "xstr, mtr, xste, mte=preprocess_and_expand(xtr,xte,degrees)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid searching the best lmbda for ridge regression\n",
    "\n",
    "# setting the search interval\n",
    "interval_size=20\n",
    "interval=np.linspace(-12, -8, interval_size)\n",
    "\n",
    "# models to test\n",
    "test_models=[r]\n",
    "test_logistics=[False]\n",
    "\n",
    "# setting the models\n",
    "logistics=[False,False,False]\n",
    "reg_fs= [r,r,r]\n",
    "datasets=3\n",
    "\n",
    "# setting model parameters\n",
    "max_iters=[100000,100000,100000] \n",
    "gammas=[10**-8,10**-8,10**-8] \n",
    "\n",
    "# initializing model hyperparameters \n",
    "lda1=10**-14\n",
    "lda2=10**-12\n",
    "lda3=10**-16\n",
    "lambdas=[lda1,lda2,lda3]\n",
    "\n",
    "# initializing the results container\n",
    "results=np.zeros((interval_size*len(test_models),3))\n",
    "\n",
    "# finding lambdas\n",
    "for data_num in range(datasets):\n",
    "    for i, m in enumerate(test_models):\n",
    "        for j, v in enumerate(interval):\n",
    "            lda1=10**v  \n",
    "            lambdas[data_num]=lda1\n",
    "            reg_fs[data_num]=m\n",
    "            logistics[data_num]=test_logistics[i]\n",
    "            performance=basic_cross(ytr, yte, xstr, mtr, xste, mte, reg_fs, lambdas, max_iters, gammas, logistics)\n",
    "            results[interval_size*i+j]=[lda1,i,performance]\n",
    "            print(\"Step \",data_num+1,\": \", 100*(i*interval_size+j+1)/(interval_size*len(test_models)), \"% done\")\n",
    "    \n",
    "    print(results[np.where(results[:,2]==np.max(results[:,2]))])\n",
    "    lambdas[data_num]=results[np.where(results[:,2]==np.max(results[:,2]))][0,0]\n",
    "    index_mod=int(results[np.where(results[:,2]==np.max(results[:,2]))][0,1])\n",
    "    reg_fs[data_num]=test_models[index_mod]\n",
    "    logistics[data_num]= test_logistics[index_mod]\n",
    "    results=np.zeros((interval_size*len(models),3))\n",
    "\n",
    "# estimating the performance of the best overall model\n",
    "performance=basic_cross(ytr, yte, xstr, mtr, xste, mte, reg_fs, lambdas, max_iters, gammas, logistics)\n",
    "print(\"Best performance: \",performance*100, \"%\") \n",
    "print(\"Best lambdas: \", lambdas)\n",
    "print(\"Best models: \", [(\"r\" if m==r else \"lsgd\") for m in reg_fs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For degree 2\n",
    "lda1=1.2*10**-15\n",
    "lda2=1.2*10**-10\n",
    "lda3=1.2*10**-14\n",
    "reg_fs=[r,r,r]\n",
    "logistics=[False,False,False]\n",
    "lambdas=[lda1,lda2,lda3]\n",
    "lambdas=[lda1,lda2,lda3]\n",
    "gamma,max_iters= [0,0,0],[0,0,0]\n",
    "\n",
    "performance=basic_cross(ytr, yte, xstr, mtr, xste, mte, reg_fs, lambdas, max_iters, gammas, logistics)\n",
    "print(\"The best performance for the degree 2 espansion is: \", performance) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For degree 3\n",
    "lda1=2.26*10**-15\n",
    "lda2=2.42*10**-12\n",
    "lda3=2.21*10**-16\n",
    "reg_fs=[r,r,r]\n",
    "logistics=[False,False,False]\n",
    "lambdas=[lda1,lda2,lda3]\n",
    "gamma,max_iters= [],[]\n",
    "\n",
    "performance=basic_cross(ytr, yte, xstr, mtr, xste, mte, reg_fs, lambdas, max_iters, gammas, logistics)\n",
    "print(\"The best performance for the degree 3 espansion is: \", performance) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
