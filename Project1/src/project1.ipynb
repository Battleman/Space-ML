{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from preprocessing import preprocessing\n",
    "from features_engineering import augment\n",
    "from implementations import *\n",
    "from cross_validation import *\n",
    "from proj1_helpers import *\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = '../data/train.csv' \n",
    "DATA_TEST_PATH = '../data/test.csv'\n",
    "y, tX_train, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding a second axis to y for dimension compatitbility reasons\n",
    "y=y[:,np.newaxis]\n",
    "\n",
    "#computing the size of the prediction to generate\n",
    "pred_len=tX_test.shape[0]\n",
    "\n",
    "#all available regression functions\n",
    "lq=lambda a,b,c,d,e,f:least_squares(a,b)\n",
    "lqgd=lambda a,b,c,d,e,f: least_squares_GD(a, b, d, e, f)\n",
    "lqsgd=lambda a,b,c,d,e,f: least_squares_SGD(a, b, d, e, f)\n",
    "r=lambda a,b,c,d,e,f: ridge_regression(a, b, c)\n",
    "lgd=lambda a,b,c,d,e,f: logistic_regression_GD(a,b,d,e,f)\n",
    "lsgd=lambda a,b,c,d,e,f: logistic_regression_SGD(a,b,d,e,f)\n",
    "rlgd=reg_logistic_regression_GD\n",
    "rlsgd=reg_logistic_regression_SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(perc,y,x):\n",
    "    \"\"\"computes a (perc, 1-perc) split of x and y\"\"\"\n",
    "    np.random.seed(seed=1)\n",
    "    sample_size=len(y)\n",
    "    cut_ind=int(perc*sample_size)\n",
    "    shuffle_indices = np.random.permutation(sample_size)\n",
    "    shuffled_x=x[shuffle_indices]\n",
    "    shuffled_y=y[shuffle_indices]\n",
    "    return shuffled_y[:cut_ind], shuffled_x[:cut_ind],shuffled_y[cut_ind:], shuffled_x[cut_ind:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_expand(xtr,xte,degrees):\n",
    "    \"\"\"preprocesses and then expands the samples\"\"\"\n",
    "    (xstr, mtr) = preprocessing(xtr)\n",
    "    (xste, mte) = preprocessing(xte)\n",
    "    for it,(degree,xtr,xte) in enumerate(zip(degrees,xstr,xste)):\n",
    "        xstr[it]=augment(xtr,degree, tan_hyp_deg=2, ilog_deg=2, root_deg=2)\n",
    "        xste[it]=augment(xte,degree, tan_hyp_deg=2, ilog_deg=2, root_deg=2)\n",
    "    return xstr, mtr, xste, mte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_labels(tx,w,logistic):\n",
    "    \"\"\"generates class predictions given weights, and a test data matrix\"\"\"\n",
    "    y_pred= 2*sigmoid(tx.dot(w))-1 if logistic else tx.dot(w)\n",
    "    y_pred[np.where(y_pred <= 0)] = -1\n",
    "    y_pred[np.where(y_pred > 0)] = 1\n",
    "    return y_pred\n",
    "\n",
    "def predictions(ys, tx, px, mask_t, mask_p, reg_fs, len_pred, lambdas, max_iters, gammas, logistics):\n",
    "    \"\"\"generates predictions using the regression function reg_f (trained on y,tx) and the inputs px\"\"\"\n",
    "    y_pred = np.zeros((len_pred,1))\n",
    "    for x_train, mask_train, x_test, mask_test, lambda_, max_iter, gamma, logistic, reg_f, y_i in zip(tx, mask_t, px, mask_p, lambdas, max_iters, gammas, logistics, reg_fs, ys):\n",
    "        #print(\"#######New subset#######\")\n",
    "        y_correspond = y_i[mask_train]\n",
    "        x_train\n",
    "        #print(\"Augmented train\")\n",
    "        initial_w= np.zeros((x_train.shape[1], 1))\n",
    "        w,_ = reg_f(y_correspond, x_train, lambda_, initial_w, max_iter, gamma)\n",
    "        #print(\"Computed weights\")\n",
    "        x_test= x_test\n",
    "        #print(\"Augmented test\")\n",
    "        y_pred[mask_test] = predict_labels(x_test,w,logistic)\n",
    "        #print(\"Computed predictions\")\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_cross(ytr, yte, xstr, mtr, xste, mte, reg_fs, lambdas, max_iters, gammas, logistics):\n",
    "    \"\"\"estimates the true performance of reg_fs through basic cross-validation between \n",
    "    a test and training set of samples\"\"\"\n",
    "    ytr_l=np.where(ytr==-1,0,1)\n",
    "    size_pred=len(yte)\n",
    "    ys=[(ytr_l if l else ytr) for l in logistics]\n",
    "    y_bar=predictions(ys, xstr, xste, mtr, mte, reg_fs, size_pred, lambdas, max_iters, gammas, logistics)\n",
    "    return np.sum((yte==y_bar))/len(yte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_kfolds(y_train, x_train, num_folds, mtr, reg_fs, lambdas, max_iters, gammas, logistics):\n",
    "    scores = []\n",
    "    #ytr_l=np.where(y_train==-1,0,1)\n",
    "    #ys=[(ytr_l if l else y_train) for l in logistics]\n",
    "    for x_sub,mask,lambda_, max_iter, gamma, logistic, reg_f in zip(x_train,mtr,lambdas,max_iters,gammas,logistics,reg_fs):\n",
    "        y_correspond = y_train[mask]\n",
    "        scores_sub = []\n",
    "        for x_train_s, x_val_s, y_train_s, y_val_s in k_fold_splits(y_correspond, x_sub, num_folds):\n",
    "            size_pred=len(y_val_s)\n",
    "            y_pred = np.zeros((size_pred,1))\n",
    "            initial_w= np.zeros((x_train_s.shape[1], 1))\n",
    "            w,_ = reg_f(y_train_s, x_train_s, lambda_, initial_w, max_iter, gamma)\n",
    "            y_pred = predict_labels(x_val_s,w,logistic)\n",
    "            score = np.mean(y_pred == y_val_s)\n",
    "            scores_sub.append(score)\n",
    "        print(\"finished subset average is :\",np.array(scores_sub).mean())\n",
    "        scores.append(np.array(scores_sub).mean())\n",
    "    return np.array(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split percentage for the cross-validation\n",
    "perc=0.8\n",
    "#generating the training and test sets\n",
    "ytr,xtr,yte,xte=split(perc,y,tX_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree 2\n",
      "Adding Roots powers\n",
      "Adding tanh powers\n",
      "Adding inverse log\n",
      "Degree 2\n",
      "Adding Roots powers\n",
      "Adding tanh powers\n",
      "Adding inverse log\n",
      "Degree 2\n",
      "Adding Roots powers\n",
      "Adding tanh powers\n",
      "Adding inverse log\n",
      "Degree 2\n",
      "Adding Roots powers\n",
      "Adding tanh powers\n",
      "Adding inverse log\n",
      "Degree 2\n",
      "Adding Roots powers\n",
      "Adding tanh powers\n",
      "Adding inverse log\n",
      "Degree 2\n",
      "Adding Roots powers\n",
      "Adding tanh powers\n",
      "Adding inverse log\n"
     ]
    }
   ],
   "source": [
    "#degrees of expansion for each of the subsets generated by the preprocessing\n",
    "degrees=[2,2,2]\n",
    "\n",
    "#preprocessing and expanding both the training and the test set\n",
    "xstr, mtr, xste, mte=preprocess_and_expand(xtr,xte,degrees)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  1 :  1.0 % done\n",
      "Step  1 :  2.0 % done\n",
      "Step  1 :  3.0 % done\n",
      "Step  1 :  4.0 % done\n",
      "Step  1 :  5.0 % done\n",
      "Step  1 :  6.0 % done\n",
      "Step  1 :  7.0 % done\n",
      "Step  1 :  8.0 % done\n",
      "Step  1 :  9.0 % done\n",
      "Step  1 :  10.0 % done\n",
      "Step  1 :  11.0 % done\n",
      "Step  1 :  12.0 % done\n",
      "Step  1 :  13.0 % done\n",
      "Step  1 :  14.0 % done\n",
      "Step  1 :  15.0 % done\n",
      "Step  1 :  16.0 % done\n",
      "Step  1 :  17.0 % done\n",
      "Step  1 :  18.0 % done\n",
      "Step  1 :  19.0 % done\n",
      "Step  1 :  20.0 % done\n",
      "Step  1 :  21.0 % done\n",
      "Step  1 :  22.0 % done\n",
      "Step  1 :  23.0 % done\n",
      "Step  1 :  24.0 % done\n",
      "Step  1 :  25.0 % done\n",
      "Step  1 :  26.0 % done\n",
      "Step  1 :  27.0 % done\n",
      "Step  1 :  28.0 % done\n",
      "Step  1 :  29.0 % done\n",
      "Step  1 :  30.0 % done\n",
      "Step  1 :  31.0 % done\n",
      "Step  1 :  32.0 % done\n",
      "Step  1 :  33.0 % done\n",
      "Step  1 :  34.0 % done\n",
      "Step  1 :  35.0 % done\n",
      "Step  1 :  36.0 % done\n",
      "Step  1 :  37.0 % done\n",
      "Step  1 :  38.0 % done\n",
      "Step  1 :  39.0 % done\n",
      "Step  1 :  40.0 % done\n",
      "Step  1 :  41.0 % done\n",
      "Step  1 :  42.0 % done\n",
      "Step  1 :  43.0 % done\n",
      "Step  1 :  44.0 % done\n",
      "Step  1 :  45.0 % done\n",
      "Step  1 :  46.0 % done\n",
      "Step  1 :  47.0 % done\n",
      "Step  1 :  48.0 % done\n",
      "Step  1 :  49.0 % done\n",
      "Step  1 :  50.0 % done\n",
      "Step  1 :  51.0 % done\n",
      "Step  1 :  52.0 % done\n",
      "Step  1 :  53.0 % done\n",
      "Step  1 :  54.0 % done\n",
      "Step  1 :  55.0 % done\n",
      "Step  1 :  56.0 % done\n",
      "Step  1 :  57.0 % done\n",
      "Step  1 :  58.0 % done\n",
      "Step  1 :  59.0 % done\n",
      "Step  1 :  60.0 % done\n",
      "Step  1 :  61.0 % done\n",
      "Step  1 :  62.0 % done\n",
      "Step  1 :  63.0 % done\n",
      "Step  1 :  64.0 % done\n",
      "Step  1 :  65.0 % done\n",
      "Step  1 :  66.0 % done\n",
      "Step  1 :  67.0 % done\n",
      "Step  1 :  68.0 % done\n",
      "Step  1 :  69.0 % done\n",
      "Step  1 :  70.0 % done\n",
      "Step  1 :  71.0 % done\n",
      "Step  1 :  72.0 % done\n",
      "Step  1 :  73.0 % done\n",
      "Step  1 :  74.0 % done\n",
      "Step  1 :  75.0 % done\n",
      "Step  1 :  76.0 % done\n",
      "Step  1 :  77.0 % done\n",
      "Step  1 :  78.0 % done\n",
      "Step  1 :  79.0 % done\n",
      "Step  1 :  80.0 % done\n",
      "Step  1 :  81.0 % done\n",
      "Step  1 :  82.0 % done\n",
      "Step  1 :  83.0 % done\n",
      "Step  1 :  84.0 % done\n",
      "Step  1 :  85.0 % done\n",
      "Step  1 :  86.0 % done\n",
      "Step  1 :  87.0 % done\n",
      "Step  1 :  88.0 % done\n",
      "Step  1 :  89.0 % done\n",
      "Step  1 :  90.0 % done\n",
      "Step  1 :  91.0 % done\n",
      "Step  1 :  92.0 % done\n",
      "Step  1 :  93.0 % done\n",
      "Step  1 :  94.0 % done\n",
      "Step  1 :  95.0 % done\n",
      "Step  1 :  96.0 % done\n",
      "Step  1 :  97.0 % done\n",
      "Step  1 :  98.0 % done\n",
      "Step  1 :  99.0 % done\n",
      "Step  1 :  100.0 % done\n",
      "[[0.00756463 0.         0.61312   ]]\n",
      "Step  2 :  1.0 % done\n",
      "Step  2 :  2.0 % done\n",
      "Step  2 :  3.0 % done\n",
      "Step  2 :  4.0 % done\n",
      "Step  2 :  5.0 % done\n",
      "Step  2 :  6.0 % done\n",
      "Step  2 :  7.0 % done\n",
      "Step  2 :  8.0 % done\n",
      "Step  2 :  9.0 % done\n",
      "Step  2 :  10.0 % done\n",
      "Step  2 :  11.0 % done\n",
      "Step  2 :  12.0 % done\n",
      "Step  2 :  13.0 % done\n",
      "Step  2 :  14.0 % done\n",
      "Step  2 :  15.0 % done\n",
      "Step  2 :  16.0 % done\n",
      "Step  2 :  17.0 % done\n",
      "Step  2 :  18.0 % done\n",
      "Step  2 :  19.0 % done\n",
      "Step  2 :  20.0 % done\n",
      "Step  2 :  21.0 % done\n",
      "Step  2 :  22.0 % done\n",
      "Step  2 :  23.0 % done\n",
      "Step  2 :  24.0 % done\n",
      "Step  2 :  25.0 % done\n",
      "Step  2 :  26.0 % done\n",
      "Step  2 :  27.0 % done\n",
      "Step  2 :  28.0 % done\n",
      "Step  2 :  29.0 % done\n",
      "Step  2 :  30.0 % done\n",
      "Step  2 :  31.0 % done\n",
      "Step  2 :  32.0 % done\n",
      "Step  2 :  33.0 % done\n",
      "Step  2 :  34.0 % done\n",
      "Step  2 :  35.0 % done\n",
      "Step  2 :  36.0 % done\n",
      "Step  2 :  37.0 % done\n",
      "Step  2 :  38.0 % done\n",
      "Step  2 :  39.0 % done\n",
      "Step  2 :  40.0 % done\n",
      "Step  2 :  41.0 % done\n",
      "Step  2 :  42.0 % done\n",
      "Step  2 :  43.0 % done\n",
      "Step  2 :  44.0 % done\n",
      "Step  2 :  45.0 % done\n",
      "Step  2 :  46.0 % done\n",
      "Step  2 :  47.0 % done\n",
      "Step  2 :  48.0 % done\n",
      "Step  2 :  49.0 % done\n",
      "Step  2 :  50.0 % done\n",
      "Step  2 :  51.0 % done\n",
      "Step  2 :  52.0 % done\n",
      "Step  2 :  53.0 % done\n",
      "Step  2 :  54.0 % done\n",
      "Step  2 :  55.0 % done\n",
      "Step  2 :  56.0 % done\n",
      "Step  2 :  57.0 % done\n",
      "Step  2 :  58.0 % done\n",
      "Step  2 :  59.0 % done\n",
      "Step  2 :  60.0 % done\n",
      "Step  2 :  61.0 % done\n",
      "Step  2 :  62.0 % done\n",
      "Step  2 :  63.0 % done\n",
      "Step  2 :  64.0 % done\n",
      "Step  2 :  65.0 % done\n",
      "Step  2 :  66.0 % done\n",
      "Step  2 :  67.0 % done\n",
      "Step  2 :  68.0 % done\n",
      "Step  2 :  69.0 % done\n",
      "Step  2 :  70.0 % done\n",
      "Step  2 :  71.0 % done\n",
      "Step  2 :  72.0 % done\n",
      "Step  2 :  73.0 % done\n",
      "Step  2 :  74.0 % done\n",
      "Step  2 :  75.0 % done\n",
      "Step  2 :  76.0 % done\n",
      "Step  2 :  77.0 % done\n",
      "Step  2 :  78.0 % done\n",
      "Step  2 :  79.0 % done\n",
      "Step  2 :  80.0 % done\n",
      "Step  2 :  81.0 % done\n",
      "Step  2 :  82.0 % done\n",
      "Step  2 :  83.0 % done\n",
      "Step  2 :  84.0 % done\n",
      "Step  2 :  85.0 % done\n",
      "Step  2 :  86.0 % done\n",
      "Step  2 :  87.0 % done\n",
      "Step  2 :  88.0 % done\n",
      "Step  2 :  89.0 % done\n",
      "Step  2 :  90.0 % done\n",
      "Step  2 :  91.0 % done\n",
      "Step  2 :  92.0 % done\n",
      "Step  2 :  93.0 % done\n",
      "Step  2 :  94.0 % done\n",
      "Step  2 :  95.0 % done\n",
      "Step  2 :  96.0 % done\n",
      "Step  2 :  97.0 % done\n",
      "Step  2 :  98.0 % done\n",
      "Step  2 :  99.0 % done\n",
      "Step  2 :  100.0 % done\n",
      "[[4.64158883e-04 0.00000000e+00 7.43220000e-01]]\n",
      "Step  3 :  1.0 % done\n",
      "Step  3 :  2.0 % done\n",
      "Step  3 :  3.0 % done\n",
      "Step  3 :  4.0 % done\n",
      "Step  3 :  5.0 % done\n",
      "Step  3 :  6.0 % done\n",
      "Step  3 :  7.0 % done\n",
      "Step  3 :  8.0 % done\n",
      "Step  3 :  9.0 % done\n",
      "Step  3 :  10.0 % done\n",
      "Step  3 :  11.0 % done\n",
      "Step  3 :  12.0 % done\n",
      "Step  3 :  13.0 % done\n",
      "Step  3 :  14.0 % done\n",
      "Step  3 :  15.0 % done\n",
      "Step  3 :  16.0 % done\n",
      "Step  3 :  17.0 % done\n",
      "Step  3 :  18.0 % done\n",
      "Step  3 :  19.0 % done\n",
      "Step  3 :  20.0 % done\n",
      "Step  3 :  21.0 % done\n",
      "Step  3 :  22.0 % done\n",
      "Step  3 :  23.0 % done\n",
      "Step  3 :  24.0 % done\n",
      "Step  3 :  25.0 % done\n",
      "Step  3 :  26.0 % done\n",
      "Step  3 :  27.0 % done\n",
      "Step  3 :  28.0 % done\n",
      "Step  3 :  29.0 % done\n",
      "Step  3 :  30.0 % done\n",
      "Step  3 :  31.0 % done\n",
      "Step  3 :  32.0 % done\n",
      "Step  3 :  33.0 % done\n",
      "Step  3 :  34.0 % done\n",
      "Step  3 :  35.0 % done\n",
      "Step  3 :  36.0 % done\n",
      "Step  3 :  37.0 % done\n",
      "Step  3 :  38.0 % done\n",
      "Step  3 :  39.0 % done\n",
      "Step  3 :  40.0 % done\n",
      "Step  3 :  41.0 % done\n",
      "Step  3 :  42.0 % done\n",
      "Step  3 :  43.0 % done\n",
      "Step  3 :  44.0 % done\n",
      "Step  3 :  45.0 % done\n",
      "Step  3 :  46.0 % done\n",
      "Step  3 :  47.0 % done\n",
      "Step  3 :  48.0 % done\n",
      "Step  3 :  49.0 % done\n",
      "Step  3 :  50.0 % done\n",
      "Step  3 :  51.0 % done\n",
      "Step  3 :  52.0 % done\n",
      "Step  3 :  53.0 % done\n",
      "Step  3 :  54.0 % done\n",
      "Step  3 :  55.0 % done\n",
      "Step  3 :  56.0 % done\n",
      "Step  3 :  57.0 % done\n",
      "Step  3 :  58.0 % done\n",
      "Step  3 :  59.0 % done\n",
      "Step  3 :  60.0 % done\n",
      "Step  3 :  61.0 % done\n",
      "Step  3 :  62.0 % done\n",
      "Step  3 :  63.0 % done\n",
      "Step  3 :  64.0 % done\n",
      "Step  3 :  65.0 % done\n",
      "Step  3 :  66.0 % done\n",
      "Step  3 :  67.0 % done\n",
      "Step  3 :  68.0 % done\n",
      "Step  3 :  69.0 % done\n",
      "Step  3 :  70.0 % done\n",
      "Step  3 :  71.0 % done\n",
      "Step  3 :  72.0 % done\n",
      "Step  3 :  73.0 % done\n",
      "Step  3 :  74.0 % done\n",
      "Step  3 :  75.0 % done\n",
      "Step  3 :  76.0 % done\n",
      "Step  3 :  77.0 % done\n",
      "Step  3 :  78.0 % done\n",
      "Step  3 :  79.0 % done\n",
      "Step  3 :  80.0 % done\n",
      "Step  3 :  81.0 % done\n",
      "Step  3 :  82.0 % done\n",
      "Step  3 :  83.0 % done\n",
      "Step  3 :  84.0 % done\n",
      "Step  3 :  85.0 % done\n",
      "Step  3 :  86.0 % done\n",
      "Step  3 :  87.0 % done\n",
      "Step  3 :  88.0 % done\n",
      "Step  3 :  89.0 % done\n",
      "Step  3 :  90.0 % done\n",
      "Step  3 :  91.0 % done\n",
      "Step  3 :  92.0 % done\n",
      "Step  3 :  93.0 % done\n",
      "Step  3 :  94.0 % done\n",
      "Step  3 :  95.0 % done\n",
      "Step  3 :  96.0 % done\n",
      "Step  3 :  97.0 % done\n",
      "Step  3 :  98.0 % done\n",
      "Step  3 :  99.0 % done\n",
      "Step  3 :  100.0 % done\n",
      "[[0.12328467 0.         0.80664   ]]\n",
      "Best performance:  80.664 %\n",
      "Best lambdas:  [0.007564633275546291, 0.00046415888336127914, 0.12328467394420735]\n",
      "Best models:  ['r', 'r', 'r']\n"
     ]
    }
   ],
   "source": [
    "# grid searching the best lmbda for ridge regression\n",
    "\n",
    "# setting the search interval\n",
    "interval_size=100\n",
    "interval=np.linspace(-30, 0, interval_size)\n",
    "\n",
    "# models to test\n",
    "test_models=[r]\n",
    "test_logistics=[False]\n",
    "\n",
    "# setting the models\n",
    "logistics=[False,False,False]\n",
    "reg_fs= [r,r,r]\n",
    "datasets=3\n",
    "\n",
    "# setting model parameters\n",
    "max_iters=[100000,100000,100000] \n",
    "gammas=[10**-8,10**-8,10**-8] \n",
    "\n",
    "# initializing model hyperparameters \n",
    "lda1=10**-14\n",
    "lda2=10**-12\n",
    "lda3=10**-16\n",
    "lambdas=[lda1,lda2,lda3]\n",
    "\n",
    "# initializing the results container\n",
    "results=np.zeros((interval_size*len(test_models),3))\n",
    "\n",
    "# finding lambdas\n",
    "for data_num in range(datasets):\n",
    "    for i, m in enumerate(test_models):\n",
    "        for j, v in enumerate(interval):\n",
    "            lda1=10**v  \n",
    "            lambdas[data_num]=lda1\n",
    "            reg_fs[data_num]=m\n",
    "            logistics[data_num]=test_logistics[i]\n",
    "            performance=basic_cross(ytr, yte, xstr, mtr, xste, mte, reg_fs, lambdas, max_iters, gammas, logistics)\n",
    "            results[interval_size*i+j]=[lda1,i,performance]\n",
    "            print(\"Step \",data_num+1,\": \", 100*(i*interval_size+j+1)/(interval_size*len(test_models)), \"% done\")\n",
    "    \n",
    "    print(results[np.where(results[:,2]==np.max(results[:,2]))])\n",
    "    lambdas[data_num]=results[np.where(results[:,2]==np.max(results[:,2]))][0,0]\n",
    "    index_mod=int(results[np.where(results[:,2]==np.max(results[:,2]))][0,1])\n",
    "    reg_fs[data_num]=test_models[index_mod]\n",
    "    logistics[data_num]= test_logistics[index_mod]\n",
    "    results=np.zeros((interval_size*len(test_models),3))\n",
    "\n",
    "# estimating the performance of the best overall model\n",
    "performance=basic_cross(ytr, yte, xstr, mtr, xste, mte, reg_fs, lambdas, max_iters, gammas, logistics)\n",
    "print(\"Best performance: \",performance*100, \"%\") \n",
    "print(\"Best lambdas: \", lambdas)\n",
    "print(\"Best models: \", [(\"r\" if m==r else \"lsgd\") for m in reg_fs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For degree 2\n",
    "lda1=1.2*10**-15\n",
    "lda2=1.2*10**-10\n",
    "lda3=1.2*10**-14\n",
    "reg_fs=[r,r,r]\n",
    "logistics=[False,False,False]\n",
    "lambdas=[lda1,lda2,lda3]\n",
    "lambdas=[lda1,lda2,lda3]\n",
    "gamma,max_iters= [0,0,0],[0,0,0]\n",
    "\n",
    "performance=basic_cross(ytr, yte, xstr, mtr, xste, mte, reg_fs, lambdas, max_iters, gammas, logistics)\n",
    "print(\"The best performance for the degree 2 espansion is: \", performance) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For degree 3\n",
    "lda1=2.26*10**-15\n",
    "lda2=2.42*10**-12\n",
    "lda3=2.21*10**-16\n",
    "reg_fs=[r,r,r]\n",
    "logistics=[False,False,False]\n",
    "lambdas=[lda1,lda2,lda3]\n",
    "gamma,max_iters= [],[]\n",
    "\n",
    "performance=basic_cross(ytr, yte, xstr, mtr, xste, mte, reg_fs, lambdas, max_iters, gammas, logistics)\n",
    "print(\"The best performance for the degree 3 espansion is: \", performance) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree 2\n",
      "Degree 3\n",
      "Degree 2\n",
      "Degree 3\n",
      "Degree 2\n",
      "Degree 3\n"
     ]
    }
   ],
   "source": [
    "#Expanding for k cross-validation\n",
    "degrees=[3,3,3]\n",
    "def preprocess_and_expandkcross(xtr,degrees):\n",
    "    \"\"\"preprocesses and then expands the samples\"\"\"\n",
    "    (xstr, mtr) = preprocessing(xtr)\n",
    "    for it,(degree,xtr) in enumerate(zip(degrees,xstr)):\n",
    "        xstr[it]=augment(xtr,degree)\n",
    "    return xstr, mtr\n",
    "\n",
    "#preprocessing and expanding both the training and the test set\n",
    "xtraink, mtr=preprocess_and_expandkcross(tX_train,degrees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished subset average is : 0.8424982484235812\n",
      "finished subset average is : 0.8003740005158628\n",
      "finished subset average is : 0.8222911497105045\n",
      "The best performance for the degree 3 espansion is:  [0.84249825 0.800374   0.82229115]\n"
     ]
    }
   ],
   "source": [
    "# For degree 3\n",
    "lda1=2.26*10**-15\n",
    "lda2=2.42*10**-12\n",
    "lda3=2.21*10**-16\n",
    "reg_fs=[r,r,r]\n",
    "logistics=[False,False,False]\n",
    "lambdas=[lda1,lda2,lda3]\n",
    "gammas,max_iters= [0,0,0],[0,0,0]\n",
    "\n",
    "performance=cross_validation_kfolds(y, xtraink, 5, mtr, reg_fs, lambdas, max_iters, gammas, logistics)\n",
    "print(\"The best performance for the degree 3 espansion is: \", performance) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
