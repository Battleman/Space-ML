{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import load_data, split_data\n",
    "import numpy as np\n",
    "import surprise as spr\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "import os\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.read_csv(\"../data/data_train.csv\")\n",
    "#extracting row and column numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Id.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(1,5))\n",
    "Dataset.load_from_df(data[[\"Row\", \"Col\", \"Prediction\"]], reader=reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/data_train.csv\") as f1:\n",
    "    with open(\"../data/data_3cols.csv\", \"wt\") as f2:\n",
    "        for l in f1.readlines()[1:]:\n",
    "            id, rating = l.split(\",\")\n",
    "            row, col = id.split(\"_\")\n",
    "            row = row[1:]\n",
    "            col = col[1:]\n",
    "            f2.write(\"{},{},{}\".format(row, col, rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_ids, all_uid, all_iid = [], [], []\n",
    "with open(\"../data/sampleSubmission.csv\") as f1:\n",
    "    for l in f1.readlines()[1:]:\n",
    "        id, _ = l.split(\",\")\n",
    "        row, col = id.split(\"_\")\n",
    "        all_uid.append(row[1:])\n",
    "        all_iid.append(col[1:])\n",
    "        raw_ids.append(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../data/data_3cols.csv'\n",
    "reader = Reader(line_format='user item rating', sep=\",\")\n",
    "data = Dataset.load_from_file(file_path, reader=reader)\n",
    "trainset = data.build_full_trainset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df = pd.read_pickle(\"cache/cached_predictions.pkl\")\n",
    "except FileNotFoundError:\n",
    "    print(\"No cached predictions found\")\n",
    "    df = pd.DataFrame(raw_ids, columns=[\"Id\"])\n",
    "    df.set_index(\"Id\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global algo_in_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ids(rid):\n",
    "    u, i = rid.split(\"_\")\n",
    "    return u[1:], i[1:]\n",
    "def predictor(ids_chunk):\n",
    "    print(\"Working on a chunk\")\n",
    "    res_chunk = []\n",
    "    for i in ids_chunk:\n",
    "        uid, iid = get_ids(i)\n",
    "        p = algo_in_use.predict(uid, iid)\n",
    "        res_chunk.append((i, p.est))\n",
    "    print(\"Finished chunk\")\n",
    "    return res_chunk\n",
    "def parallelize_predictions(ids, algo, n_cores=16):\n",
    "    splitted_ids = np.array_split(ids, n_cores)\n",
    "    pool = Pool(n_cores)\n",
    "    res = np.concatenate(pool.map(predictor, splitted_ids))\n",
    "    res = [(r[0], float(r[1])) for r in res]\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_algos = {\"SVD\": spr.SVD(), \"Baseline\": spr.BaselineOnly(), \"NMF\": spr.NMF(), \n",
    "             \"Slope One\": spr.SlopeOne(), \"KNN Basic\": spr.KNNBasic(), \n",
    "             \"KNN Means\": spr.KNNWithMeans(), \"KNN Baseline\": spr.KNNBaseline(), \n",
    "             \"KNN Zscore\":spr.KNNWithZScore(), \"SVD ++\": spr.SVDpp()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting loop\")\n",
    "for name in all_algos:\n",
    "    print(\"##### {} ####\".format(name))\n",
    "    if name in df.columns:\n",
    "        print(\"Already computed {}, skipping\".format(name))\n",
    "        continue\n",
    "    algo = all_algos[name]\n",
    "    time.sleep(1)\n",
    "    algo.fit(trainset)\n",
    "    time.sleep(1)\n",
    "    algo_in_use = algo\n",
    "    print(\"Generating predictions...\")\n",
    "    predictions = parallelize_predictions(raw_ids, algo, 80)\n",
    "    print(\"Done. Merging with previous results\")\n",
    "    pred_df = pd.DataFrame(predictions, columns=[\"Id\", name])\n",
    "    pred_df.set_index(\"Id\", inplace=True)\n",
    "    df = pd.merge(df, pred_df, left_index=True, right_index=True)\n",
    "    df.to_pickle(\"cache/cached_predictions.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
