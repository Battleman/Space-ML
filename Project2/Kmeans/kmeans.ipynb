{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Recommender System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn import mixture\n",
    "from notebook_helpers import *\n",
    "from scipy import sparse\n",
    "from plots import *\n",
    "import scipy.sparse as sp\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>r44_c1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>r61_c1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>r67_c1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>r72_c1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>r86_c1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id  Prediction\n",
       "0  r44_c1           4\n",
       "1  r61_c1           3\n",
       "2  r67_c1           4\n",
       "3  r72_c1           3\n",
       "4  r86_c1           5"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading the training data\n",
    "data=pd.read_csv('../data/data_train.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[44, 1]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[61, 1]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[67, 1]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[72, 1]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[86, 1]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id  Prediction\n",
       "0  [44, 1]           4\n",
       "1  [61, 1]           3\n",
       "2  [67, 1]           4\n",
       "3  [72, 1]           3\n",
       "4  [86, 1]           5"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting row and column numbers\n",
    "data['Id']=data['Id'].apply(lambda x: re.findall(r'\\d+', str(x)))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Row</th>\n",
       "      <th>Col</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>86</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Prediction Row Col\n",
       "0           4  44   1\n",
       "1           3  61   1\n",
       "2           4  67   1\n",
       "3           3  72   1\n",
       "4           5  86   1"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Row and column values to features\n",
    "num_id_data=data\n",
    "num_id_data[['Row', 'Col']]=pd.DataFrame(data.Id.values.tolist(), index= data.index)\n",
    "num_id_data=num_id_data.drop(columns='Id')\n",
    "num_id_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Col</th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Row</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Col     1   10  100  1000  101  102  103  104  105  106  ...  990  991  992  \\\n",
       "Row                                                      ...                  \n",
       "1     NaN  5.0  NaN   NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  NaN  NaN  NaN   \n",
       "10    NaN  NaN  NaN   NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  NaN  NaN  NaN   \n",
       "100   NaN  5.0  NaN   NaN  5.0  NaN  3.0  2.0  2.0  NaN  ...  NaN  NaN  NaN   \n",
       "1000  NaN  2.0  4.0   2.0  5.0  NaN  NaN  NaN  4.0  NaN  ...  NaN  NaN  4.0   \n",
       "10000 NaN  4.0  NaN   3.0  NaN  NaN  NaN  NaN  NaN  3.0  ...  NaN  2.0  NaN   \n",
       "\n",
       "Col    993  994  995  996  997  998  999  \n",
       "Row                                       \n",
       "1      NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "10     NaN  NaN  5.0  NaN  4.0  NaN  NaN  \n",
       "100    NaN  NaN  1.0  NaN  NaN  NaN  NaN  \n",
       "1000   2.0  NaN  5.0  1.0  NaN  2.0  3.0  \n",
       "10000  NaN  NaN  NaN  3.0  3.0  NaN  NaN  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cli_vs_mov=num_id_data.pivot(index='Row', columns='Col', values='Prediction')\n",
    "cli_vs_mov.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_clusters(data, k):\n",
    "    \"\"\"initialize the k cluster centers (the means).\n",
    "    input:\n",
    "        data: original data with shape (num_sample, num_feature).\n",
    "        k: predefined number of clusters for the k-means algorithm.\n",
    "    output:\n",
    "        a numpy array with shape (k, num_feature)\n",
    "    \"\"\"\n",
    "    # ***************************************************************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: initialize cluster centers.\n",
    "    # TODO: Make sure you choose k clusters from the data itself,\n",
    "    #       or ensure otherwise that your initializations have the same scaling as the data\n",
    "    # ***************************************************************************************************\n",
    "    #np.random.seed(1)\n",
    "    \n",
    "    #min_data=np.nanmin(data,axis=0)\n",
    "    #max_data=np.nanmax(data,axis=0)\n",
    "    #return np.random.uniform(min_data,max_data,(k,data.shape[1]))\n",
    "    return np.zeros((k,data.shape[1]))+3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_distance_matrix(data, mu):\n",
    "    \"\"\"build a distance matrix.\n",
    "    return\n",
    "        distance matrix:\n",
    "            row of the matrix represents the data point,\n",
    "            column of the matrix represents the k-th cluster.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: build distance matrix\n",
    "    # ***************************************************\n",
    "    return np.array([np.nansum((data-mu[i])**2,axis=1) for i in range(len(mu))]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_kmeans_parameters(data, mu_old):\n",
    "    \"\"\"update the parameter of kmeans\n",
    "    return:\n",
    "        losses: loss of each data point with shape (num_samples, 1)\n",
    "        assignments: assignments vector z with shape (num_samples, 1)\n",
    "        mu: mean vector mu with shape (k, num_features)\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: update kmeans parameters\n",
    "    # ***************************************************\n",
    "    d=build_distance_matrix(data, mu_old)\n",
    "    losses=np.min(d,1)\n",
    "    assignments=np.argmin(d,1)\n",
    "    mu=np.array([np.mean(data[assignments==i],0) for i in range(mu_old.shape[0])])\n",
    "    mu=np.where(np.isnan(mu),mu_old,mu)\n",
    "    return losses,assignments,mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(data, k, max_iters, threshold):\n",
    "    \"\"\"run the k-means algorithm.\"\"\"\n",
    "    output_figure = \"kmeans_figures/\"\n",
    "    #Initialize the cluster.\n",
    "    mu_old = initialize_clusters(data, k)\n",
    "    average_loss=0\n",
    "    #Start the kmeans algorithm.\n",
    "    for iter in range(max_iters):\n",
    "        #Update z and mu\n",
    "        losses, assignments, mu = update_kmeans_parameters(data, mu_old)\n",
    "        #Calculate the average loss over all points\n",
    "        old_avg_loss=average_loss\n",
    "        average_loss = np.mean(losses)\n",
    "        print(\"The current iteration of k-means is: {i}, \\\n",
    "               the average loss is {l}.\".format(i=iter, l=average_loss))\n",
    "        #Check converge\n",
    "        if iter > 0 and np.abs(average_loss - old_avg_loss) < threshold:\n",
    "            break\n",
    "        #Update k-means information.\n",
    "        mu_old = mu\n",
    "    return assignments, mu, average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns the most common value for each column of n\n",
    "def most_common(x):\n",
    "    top_freq=[]\n",
    "    for i in range(x.shape[1]):\n",
    "        temp=np.unique(x[:,i],return_counts=True)\n",
    "        top_freq=np.append(top_freq,temp[0][temp[1].argmax()])\n",
    "    return top_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splits the data between a train and a test dataset\n",
    "def split_data(ratings, num_items_per_user, num_users_per_item,\n",
    "               min_num_ratings, p_test=0.1):\n",
    "    \"\"\"split the ratings to training data and test data.\n",
    "    Args:\n",
    "        min_num_ratings: \n",
    "            all users and items we keep must have at least min_num_ratings per user and per item. \n",
    "    \"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(988)\n",
    "    \n",
    "    # select user and item based on the condition.\n",
    "    valid_users = np.where(num_items_per_user >= min_num_ratings)[0]\n",
    "    valid_items = np.where(num_users_per_item >= min_num_ratings)[0]\n",
    "    valid_ratings = ratings[valid_items, :][: , valid_users]  \n",
    "    \n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # split the data and return train and test data. TODO\n",
    "    # NOTE: we only consider users and movies that have more\n",
    "    # than 10 ratings\n",
    "    # ***************************************************\n",
    "    \n",
    "    train=sp.lil_matrix(valid_ratings.shape)\n",
    "    test=sp.lil_matrix(valid_ratings.shape)\n",
    "    x,_=valid_ratings.nonzero()\n",
    "    x=np.unique(x)\n",
    "    for j,row in enumerate(x):\n",
    "        if j%10000==0:\n",
    "            print(j/len(x))\n",
    "        _,y=valid_ratings[row].nonzero()\n",
    "        perm_indices=np.random.permutation(range(len(y)))\n",
    "        y_shuffled=y[perm_indices]\n",
    "        bound=int(p_test*y_shuffled.shape[0])\n",
    "        for i,col in enumerate(y_shuffled):\n",
    "            if i<=bound:\n",
    "                test[row,col]=valid_ratings[row,col]\n",
    "            else:\n",
    "                train[row,col]=valid_ratings[row,col]\n",
    "        \n",
    "    print(\"Total number of nonzero elements in origial data:{v}\".format(v=ratings.nnz))\n",
    "    print(\"Total number of nonzero elements in train data:{v}\".format(v=train.nnz))\n",
    "    print(\"Total number of nonzero elements in test data:{v}\".format(v=test.nnz))\n",
    "    return valid_ratings, train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-Cluster Aggregation Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating a prediction using the clusters of similar users\n",
    "#PS:Yields the best results\n",
    "def cluster_agg(assignments, mu, k, train):\n",
    "    #Rounding the cluster values to get valid ratings\n",
    "    mu_rounded=np.round(mu)\n",
    "    #Computing the resulting rating matrix\n",
    "    prediction=train.copy()\n",
    "    for j in range(k):\n",
    "        prediction[assignments==j]=np.where(np.isnan(prediction[assignments==j]),mu_rounded[j],prediction[assignments==j])   \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating a prediction using the median of the scores of similar users\n",
    "def median_agg(assignments, mu, k, train):\n",
    "    #Computing the resulting rating matrix\n",
    "    prediction=train.copy().to_numpy()\n",
    "    #Computing the general median per column\n",
    "    median=np.median(prediction,0)[np.newaxis,:]\n",
    "    for j in range(k):\n",
    "        #Computing the medians of cluster j\n",
    "        medians_j=np.rint(np.nanmedian(np.concatenate((prediction[assignments==j],median),0),0))\n",
    "        #Assigning the median to the unkown ratings\n",
    "        prediction[assignments==j]=np.where(np.isnan(prediction[assignments==j]),medians_j,prediction[assignments==j])\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Generating a prediction using the most frequent score among similar users\n",
    "def mode_agg(assignments, mu, k, train):\n",
    "    #Computing the resulting rating matrix\n",
    "    prediction=train.copy().to_numpy()\n",
    "    #Computing the general mode per column\n",
    "    mode=most_common(prediction)\n",
    "    for j in range(k):\n",
    "        #Computing the modes of cluster j\n",
    "        modes_j=most_common(prediction[assignments==j])\n",
    "        modes_j=np.where(np.isnan(modes_j),mode,modes_j)\n",
    "        #Assigning the median to the unkown ratings\n",
    "        prediction[assignments==j]=np.where(np.isnan(prediction[assignments==j]),modes_j,prediction[assignments==j])\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-Parameter Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "Total number of nonzero elements in origial data:1176952\n",
      "Total number of nonzero elements in train data:1053792\n",
      "Total number of nonzero elements in test data:123160\n"
     ]
    }
   ],
   "source": [
    "#Modifying the data for compatibility reasons\n",
    "ratings=cli_vs_mov.copy()\n",
    "ratings[np.isnan(ratings)]=0\n",
    "ratings=sparse.csr_matrix(ratings)\n",
    "#Splitting the data between a train and a test dataset\n",
    "num_items_per_user, num_users_per_item = plot_raw_data(ratings)\n",
    "valid_ratings, train, test = split_data(\n",
    "    ratings, num_items_per_user, num_users_per_item, min_num_ratings=0, p_test=0.1)\n",
    "#Modifying the train and test data for compatibility reasons\n",
    "train=pd.DataFrame(train.todense())\n",
    "train[train==0]=np.nan\n",
    "test_size=test.count_nonzero()\n",
    "test=np.asarray(test.todense())\n",
    "test[test==0]=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For k= 1 :\n",
      "The current iteration of k-means is: 0,                the average loss is 209.3436.\n",
      "The current iteration of k-means is: 1,                the average loss is 111.70627300168742.\n",
      "The current iteration of k-means is: 2,                the average loss is 111.70627300168742.\n",
      "For k= 2 :\n",
      "The current iteration of k-means is: 0,                the average loss is 209.3436.\n",
      "The current iteration of k-means is: 1,                the average loss is 110.66320349076125.\n",
      "The current iteration of k-means is: 2,                the average loss is 108.98123160852803.\n",
      "The current iteration of k-means is: 3,                the average loss is 107.78747257804835.\n",
      "The current iteration of k-means is: 4,                the average loss is 107.09911560939705.\n",
      "The current iteration of k-means is: 5,                the average loss is 106.7146176701663.\n",
      "The current iteration of k-means is: 6,                the average loss is 106.52564833846745.\n",
      "The current iteration of k-means is: 7,                the average loss is 106.4382625666724.\n",
      "The current iteration of k-means is: 8,                the average loss is 106.39569376096942.\n",
      "The current iteration of k-means is: 9,                the average loss is 106.372344950803.\n",
      "The current iteration of k-means is: 10,                the average loss is 106.35736837422544.\n",
      "The current iteration of k-means is: 11,                the average loss is 106.35085008203711.\n",
      "The current iteration of k-means is: 12,                the average loss is 106.34406167190077.\n",
      "The current iteration of k-means is: 13,                the average loss is 106.34128675427391.\n",
      "The current iteration of k-means is: 14,                the average loss is 106.33904198501465.\n",
      "The current iteration of k-means is: 15,                the average loss is 106.3369760278157.\n",
      "The current iteration of k-means is: 16,                the average loss is 106.33559736357874.\n",
      "The current iteration of k-means is: 17,                the average loss is 106.33487019857185.\n",
      "For k= 3 :\n",
      "The current iteration of k-means is: 0,                the average loss is 209.3436.\n",
      "The current iteration of k-means is: 1,                the average loss is 110.66320349076125.\n",
      "The current iteration of k-means is: 2,                the average loss is 108.95197315927015.\n",
      "The current iteration of k-means is: 3,                the average loss is 107.58972785990183.\n",
      "The current iteration of k-means is: 4,                the average loss is 106.83424964917532.\n",
      "The current iteration of k-means is: 5,                the average loss is 106.38989972828561.\n",
      "The current iteration of k-means is: 6,                the average loss is 106.16962534657775.\n",
      "The current iteration of k-means is: 7,                the average loss is 106.04236532575351.\n",
      "The current iteration of k-means is: 8,                the average loss is 105.94007332350249.\n",
      "The current iteration of k-means is: 9,                the average loss is 105.83823414601908.\n",
      "The current iteration of k-means is: 10,                the average loss is 105.73028607462682.\n",
      "The current iteration of k-means is: 11,                the average loss is 105.60816780603605.\n",
      "The current iteration of k-means is: 12,                the average loss is 105.46111420688014.\n",
      "The current iteration of k-means is: 13,                the average loss is 105.28213643243394.\n",
      "The current iteration of k-means is: 14,                the average loss is 105.11286340999645.\n",
      "The current iteration of k-means is: 15,                the average loss is 104.99334831885041.\n",
      "The current iteration of k-means is: 16,                the average loss is 104.90742357266767.\n",
      "The current iteration of k-means is: 17,                the average loss is 104.84095929050547.\n",
      "The current iteration of k-means is: 18,                the average loss is 104.79638646371481.\n",
      "The current iteration of k-means is: 19,                the average loss is 104.75389385779002.\n",
      "The current iteration of k-means is: 20,                the average loss is 104.72676607403763.\n",
      "The current iteration of k-means is: 21,                the average loss is 104.70403223421978.\n",
      "The current iteration of k-means is: 22,                the average loss is 104.68390671633182.\n",
      "The current iteration of k-means is: 23,                the average loss is 104.66823969581729.\n",
      "The current iteration of k-means is: 24,                the average loss is 104.65722602131765.\n",
      "The current iteration of k-means is: 25,                the average loss is 104.64948505213673.\n",
      "The current iteration of k-means is: 26,                the average loss is 104.64152123274938.\n",
      "The current iteration of k-means is: 27,                the average loss is 104.63639173083155.\n",
      "The current iteration of k-means is: 28,                the average loss is 104.63202230023374.\n",
      "The current iteration of k-means is: 29,                the average loss is 104.62760779013468.\n",
      "The current iteration of k-means is: 30,                the average loss is 104.62343482113633.\n",
      "The current iteration of k-means is: 31,                the average loss is 104.6206596724896.\n",
      "The current iteration of k-means is: 32,                the average loss is 104.6185415593861.\n",
      "The current iteration of k-means is: 33,                the average loss is 104.61705816813999.\n",
      "The current iteration of k-means is: 34,                the average loss is 104.61596053025971.\n",
      "The current iteration of k-means is: 35,                the average loss is 104.61434660377026.\n",
      "The current iteration of k-means is: 36,                the average loss is 104.61333469254599.\n",
      "The current iteration of k-means is: 37,                the average loss is 104.61246825148756.\n",
      "For k= 4 :\n",
      "The current iteration of k-means is: 0,                the average loss is 209.3436.\n",
      "The current iteration of k-means is: 1,                the average loss is 110.66320349076125.\n",
      "The current iteration of k-means is: 2,                the average loss is 108.95197315927015.\n",
      "The current iteration of k-means is: 3,                the average loss is 107.58116843682721.\n",
      "The current iteration of k-means is: 4,                the average loss is 106.70744788129824.\n",
      "The current iteration of k-means is: 5,                the average loss is 106.2474734463742.\n",
      "The current iteration of k-means is: 6,                the average loss is 105.97954788125686.\n",
      "The current iteration of k-means is: 7,                the average loss is 105.80301701429545.\n",
      "The current iteration of k-means is: 8,                the average loss is 105.7043912462561.\n",
      "The current iteration of k-means is: 9,                the average loss is 105.63536919576177.\n",
      "The current iteration of k-means is: 10,                the average loss is 105.48016237430805.\n",
      "The current iteration of k-means is: 11,                the average loss is 105.31479301362384.\n",
      "The current iteration of k-means is: 12,                the average loss is 105.18561965597547.\n",
      "The current iteration of k-means is: 13,                the average loss is 105.02502599783324.\n",
      "The current iteration of k-means is: 14,                the average loss is 104.85883144444986.\n",
      "The current iteration of k-means is: 15,                the average loss is 104.70968086726837.\n",
      "The current iteration of k-means is: 16,                the average loss is 104.55845433902464.\n",
      "The current iteration of k-means is: 17,                the average loss is 104.40844273603558.\n",
      "The current iteration of k-means is: 18,                the average loss is 104.27052293211999.\n",
      "The current iteration of k-means is: 19,                the average loss is 104.1660278915641.\n",
      "The current iteration of k-means is: 20,                the average loss is 104.06708114217986.\n",
      "The current iteration of k-means is: 21,                the average loss is 103.98469292740934.\n",
      "The current iteration of k-means is: 22,                the average loss is 103.91697084751108.\n",
      "The current iteration of k-means is: 23,                the average loss is 103.86348344511694.\n",
      "The current iteration of k-means is: 24,                the average loss is 103.81058816342288.\n",
      "The current iteration of k-means is: 25,                the average loss is 103.76644990761193.\n",
      "The current iteration of k-means is: 26,                the average loss is 103.7290519715074.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current iteration of k-means is: 27,                the average loss is 103.6949895402669.\n",
      "The current iteration of k-means is: 28,                the average loss is 103.66693786266084.\n",
      "The current iteration of k-means is: 29,                the average loss is 103.641214092165.\n",
      "The current iteration of k-means is: 30,                the average loss is 103.61630115000301.\n",
      "The current iteration of k-means is: 31,                the average loss is 103.59880007871399.\n",
      "The current iteration of k-means is: 32,                the average loss is 103.58086606076112.\n",
      "The current iteration of k-means is: 33,                the average loss is 103.56426763716935.\n",
      "The current iteration of k-means is: 34,                the average loss is 103.5491003779117.\n",
      "The current iteration of k-means is: 35,                the average loss is 103.53866645917681.\n",
      "The current iteration of k-means is: 36,                the average loss is 103.52810480020895.\n",
      "The current iteration of k-means is: 37,                the average loss is 103.51848588575432.\n",
      "The current iteration of k-means is: 38,                the average loss is 103.50955601476024.\n",
      "The current iteration of k-means is: 39,                the average loss is 103.50317818603352.\n",
      "The current iteration of k-means is: 40,                the average loss is 103.49567506172157.\n",
      "The current iteration of k-means is: 41,                the average loss is 103.49002243574105.\n",
      "The current iteration of k-means is: 42,                the average loss is 103.4843703927062.\n",
      "The current iteration of k-means is: 43,                the average loss is 103.47770336310069.\n",
      "The current iteration of k-means is: 44,                the average loss is 103.47118333060463.\n",
      "The current iteration of k-means is: 45,                the average loss is 103.46578953605403.\n",
      "The current iteration of k-means is: 46,                the average loss is 103.46192497517244.\n",
      "The current iteration of k-means is: 47,                the average loss is 103.45901701902108.\n",
      "The current iteration of k-means is: 48,                the average loss is 103.45636253431367.\n",
      "The current iteration of k-means is: 49,                the average loss is 103.45248131979636.\n",
      "The current iteration of k-means is: 50,                the average loss is 103.44784168527985.\n",
      "The current iteration of k-means is: 51,                the average loss is 103.44199040020597.\n",
      "The current iteration of k-means is: 52,                the average loss is 103.43839979481493.\n",
      "The current iteration of k-means is: 53,                the average loss is 103.43501171686492.\n",
      "The current iteration of k-means is: 54,                the average loss is 103.43170621623587.\n",
      "The current iteration of k-means is: 55,                the average loss is 103.42684383294896.\n",
      "The current iteration of k-means is: 56,                the average loss is 103.4228859829439.\n",
      "The current iteration of k-means is: 57,                the average loss is 103.41941976926667.\n",
      "The current iteration of k-means is: 58,                the average loss is 103.41718996024525.\n",
      "The current iteration of k-means is: 59,                the average loss is 103.41516320375747.\n",
      "The current iteration of k-means is: 60,                the average loss is 103.41275185303807.\n",
      "The current iteration of k-means is: 61,                the average loss is 103.41094279079954.\n",
      "The current iteration of k-means is: 62,                the average loss is 103.40841989267112.\n",
      "The current iteration of k-means is: 63,                the average loss is 103.40621076273553.\n",
      "The current iteration of k-means is: 64,                the average loss is 103.4033614048357.\n",
      "The current iteration of k-means is: 65,                the average loss is 103.4015175332943.\n",
      "The current iteration of k-means is: 66,                the average loss is 103.40041535047698.\n",
      "The current iteration of k-means is: 67,                the average loss is 103.39913592258728.\n",
      "The current iteration of k-means is: 68,                the average loss is 103.39819392266234.\n",
      "For k= 5 :\n",
      "The current iteration of k-means is: 0,                the average loss is 209.3436.\n",
      "The current iteration of k-means is: 1,                the average loss is 110.66320349076125.\n",
      "The current iteration of k-means is: 2,                the average loss is 108.95197315927015.\n",
      "The current iteration of k-means is: 3,                the average loss is 107.58116843682721.\n",
      "The current iteration of k-means is: 4,                the average loss is 106.6947082535944.\n",
      "The current iteration of k-means is: 5,                the average loss is 106.03414180324462.\n",
      "The current iteration of k-means is: 6,                the average loss is 105.68720819515273.\n",
      "The current iteration of k-means is: 7,                the average loss is 105.46066713877696.\n",
      "The current iteration of k-means is: 8,                the average loss is 105.30367541910233.\n",
      "The current iteration of k-means is: 9,                the average loss is 105.20694705983512.\n",
      "The current iteration of k-means is: 10,                the average loss is 105.10370580020899.\n",
      "The current iteration of k-means is: 11,                the average loss is 104.979543023306.\n",
      "The current iteration of k-means is: 12,                the average loss is 104.81027208775188.\n",
      "The current iteration of k-means is: 13,                the average loss is 104.6512876025113.\n",
      "The current iteration of k-means is: 14,                the average loss is 104.48656187562041.\n",
      "The current iteration of k-means is: 15,                the average loss is 104.33466163498801.\n",
      "The current iteration of k-means is: 16,                the average loss is 104.19103346478066.\n",
      "The current iteration of k-means is: 17,                the average loss is 104.0482280457397.\n",
      "The current iteration of k-means is: 18,                the average loss is 103.88869824586384.\n",
      "The current iteration of k-means is: 19,                the average loss is 103.7582276931027.\n",
      "The current iteration of k-means is: 20,                the average loss is 103.64069005488443.\n",
      "The current iteration of k-means is: 21,                the average loss is 103.54865590152666.\n",
      "The current iteration of k-means is: 22,                the average loss is 103.46699279434222.\n",
      "The current iteration of k-means is: 23,                the average loss is 103.4025863784815.\n",
      "The current iteration of k-means is: 24,                the average loss is 103.35715154883356.\n",
      "The current iteration of k-means is: 25,                the average loss is 103.31278259854145.\n",
      "The current iteration of k-means is: 26,                the average loss is 103.27706235476536.\n",
      "The current iteration of k-means is: 27,                the average loss is 103.24881566523814.\n",
      "The current iteration of k-means is: 28,                the average loss is 103.22746920246215.\n",
      "The current iteration of k-means is: 29,                the average loss is 103.20898090512091.\n",
      "The current iteration of k-means is: 30,                the average loss is 103.19508576874608.\n",
      "The current iteration of k-means is: 31,                the average loss is 103.18461262600107.\n",
      "The current iteration of k-means is: 32,                the average loss is 103.17665253515716.\n",
      "The current iteration of k-means is: 33,                the average loss is 103.16885919127945.\n",
      "The current iteration of k-means is: 34,                the average loss is 103.16096605342415.\n",
      "The current iteration of k-means is: 35,                the average loss is 103.15479902321438.\n",
      "The current iteration of k-means is: 36,                the average loss is 103.14927960737761.\n",
      "The current iteration of k-means is: 37,                the average loss is 103.14471244430332.\n",
      "The current iteration of k-means is: 38,                the average loss is 103.14223260392629.\n",
      "The current iteration of k-means is: 39,                the average loss is 103.13955420941859.\n",
      "The current iteration of k-means is: 40,                the average loss is 103.1371192704555.\n",
      "The current iteration of k-means is: 41,                the average loss is 103.13496003538735.\n",
      "The current iteration of k-means is: 42,                the average loss is 103.13300397025999.\n",
      "The current iteration of k-means is: 43,                the average loss is 103.13109713807182.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current iteration of k-means is: 44,                the average loss is 103.12893529535656.\n",
      "The current iteration of k-means is: 45,                the average loss is 103.12610282936382.\n",
      "The current iteration of k-means is: 46,                the average loss is 103.124189219989.\n",
      "The current iteration of k-means is: 47,                the average loss is 103.12339990603202.\n",
      "For k= 6 :\n",
      "The current iteration of k-means is: 0,                the average loss is 209.3436.\n",
      "The current iteration of k-means is: 1,                the average loss is 110.66320349076125.\n",
      "The current iteration of k-means is: 2,                the average loss is 108.95197315927015.\n",
      "The current iteration of k-means is: 3,                the average loss is 107.58116843682721.\n",
      "The current iteration of k-means is: 4,                the average loss is 106.6947082535944.\n",
      "The current iteration of k-means is: 5,                the average loss is 106.02647544430306.\n",
      "The current iteration of k-means is: 6,                the average loss is 105.50968969277409.\n",
      "The current iteration of k-means is: 7,                the average loss is 105.28392452632023.\n",
      "The current iteration of k-means is: 8,                the average loss is 105.13490391936948.\n",
      "The current iteration of k-means is: 9,                the average loss is 105.03445704717059.\n",
      "The current iteration of k-means is: 10,                the average loss is 104.94747580798776.\n",
      "The current iteration of k-means is: 11,                the average loss is 104.86184402001966.\n",
      "The current iteration of k-means is: 12,                the average loss is 104.779323607818.\n",
      "The current iteration of k-means is: 13,                the average loss is 104.68486253268294.\n",
      "The current iteration of k-means is: 14,                the average loss is 104.55050771422998.\n",
      "The current iteration of k-means is: 15,                the average loss is 104.39998772229092.\n",
      "The current iteration of k-means is: 16,                the average loss is 104.25186523849128.\n",
      "The current iteration of k-means is: 17,                the average loss is 104.13730714000684.\n",
      "The current iteration of k-means is: 18,                the average loss is 104.0062828073455.\n",
      "The current iteration of k-means is: 19,                the average loss is 103.86274239419619.\n",
      "The current iteration of k-means is: 20,                the average loss is 103.7161873870668.\n",
      "The current iteration of k-means is: 21,                the average loss is 103.57291596887889.\n",
      "The current iteration of k-means is: 22,                the average loss is 103.44784842682654.\n",
      "The current iteration of k-means is: 23,                the average loss is 103.3370150750524.\n",
      "The current iteration of k-means is: 24,                the average loss is 103.23413104183753.\n",
      "The current iteration of k-means is: 25,                the average loss is 103.15022458804798.\n",
      "The current iteration of k-means is: 26,                the average loss is 103.08762609808423.\n",
      "The current iteration of k-means is: 27,                the average loss is 103.04099269953187.\n",
      "The current iteration of k-means is: 28,                the average loss is 103.00633806296938.\n",
      "The current iteration of k-means is: 29,                the average loss is 102.97418129640587.\n",
      "The current iteration of k-means is: 30,                the average loss is 102.94579831085801.\n",
      "The current iteration of k-means is: 31,                the average loss is 102.92227536215081.\n",
      "The current iteration of k-means is: 32,                the average loss is 102.90356009143784.\n",
      "The current iteration of k-means is: 33,                the average loss is 102.88719425712421.\n",
      "The current iteration of k-means is: 34,                the average loss is 102.86766670286869.\n",
      "The current iteration of k-means is: 35,                the average loss is 102.8534527132734.\n",
      "The current iteration of k-means is: 36,                the average loss is 102.83896428610477.\n",
      "The current iteration of k-means is: 37,                the average loss is 102.82369136598074.\n",
      "The current iteration of k-means is: 38,                the average loss is 102.80756928796227.\n",
      "The current iteration of k-means is: 39,                the average loss is 102.79461045673993.\n",
      "The current iteration of k-means is: 40,                the average loss is 102.78457804688551.\n",
      "The current iteration of k-means is: 41,                the average loss is 102.77428232069094.\n",
      "The current iteration of k-means is: 42,                the average loss is 102.76244677379283.\n",
      "The current iteration of k-means is: 43,                the average loss is 102.7545010419615.\n",
      "The current iteration of k-means is: 44,                the average loss is 102.74399757625737.\n",
      "The current iteration of k-means is: 45,                the average loss is 102.73577315286892.\n",
      "The current iteration of k-means is: 46,                the average loss is 102.72700896811578.\n",
      "The current iteration of k-means is: 47,                the average loss is 102.71777804743371.\n",
      "The current iteration of k-means is: 48,                the average loss is 102.70879943938469.\n",
      "The current iteration of k-means is: 49,                the average loss is 102.70201513941466.\n",
      "The current iteration of k-means is: 50,                the average loss is 102.69573595479042.\n",
      "The current iteration of k-means is: 51,                the average loss is 102.68953063694526.\n",
      "The current iteration of k-means is: 52,                the average loss is 102.68561436054301.\n",
      "The current iteration of k-means is: 53,                the average loss is 102.68116281488818.\n",
      "The current iteration of k-means is: 54,                the average loss is 102.67489321754522.\n",
      "The current iteration of k-means is: 55,                the average loss is 102.66706659782824.\n",
      "The current iteration of k-means is: 56,                the average loss is 102.65987007307363.\n",
      "The current iteration of k-means is: 57,                the average loss is 102.65349278108816.\n",
      "The current iteration of k-means is: 58,                the average loss is 102.64553349619507.\n",
      "The current iteration of k-means is: 59,                the average loss is 102.64015032846494.\n",
      "The current iteration of k-means is: 60,                the average loss is 102.6349681507127.\n",
      "The current iteration of k-means is: 61,                the average loss is 102.63133208176451.\n",
      "The current iteration of k-means is: 62,                the average loss is 102.62680453770737.\n",
      "The current iteration of k-means is: 63,                the average loss is 102.62123313905185.\n",
      "The current iteration of k-means is: 64,                the average loss is 102.61283897407216.\n",
      "The current iteration of k-means is: 65,                the average loss is 102.60067890665265.\n",
      "The current iteration of k-means is: 66,                the average loss is 102.58502926982464.\n",
      "The current iteration of k-means is: 67,                the average loss is 102.5684662760997.\n",
      "The current iteration of k-means is: 68,                the average loss is 102.54769778324868.\n",
      "The current iteration of k-means is: 69,                the average loss is 102.52633098241834.\n",
      "The current iteration of k-means is: 70,                the average loss is 102.50296414790394.\n",
      "The current iteration of k-means is: 71,                the average loss is 102.46764357495556.\n",
      "The current iteration of k-means is: 72,                the average loss is 102.42853653858171.\n",
      "The current iteration of k-means is: 73,                the average loss is 102.38652884705918.\n",
      "The current iteration of k-means is: 74,                the average loss is 102.34915411491892.\n",
      "The current iteration of k-means is: 75,                the average loss is 102.31509097921159.\n",
      "The current iteration of k-means is: 76,                the average loss is 102.27988682446139.\n",
      "The current iteration of k-means is: 77,                the average loss is 102.24574037182563.\n",
      "The current iteration of k-means is: 78,                the average loss is 102.21362959801881.\n",
      "The current iteration of k-means is: 79,                the average loss is 102.18297754260162.\n",
      "The current iteration of k-means is: 80,                the average loss is 102.15803731812025.\n",
      "The current iteration of k-means is: 81,                the average loss is 102.14201322129578.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current iteration of k-means is: 82,                the average loss is 102.12752910046198.\n",
      "The current iteration of k-means is: 83,                the average loss is 102.11363305825445.\n",
      "The current iteration of k-means is: 84,                the average loss is 102.09965836297935.\n",
      "The current iteration of k-means is: 85,                the average loss is 102.08863636553835.\n",
      "The current iteration of k-means is: 86,                the average loss is 102.08119980477937.\n",
      "The current iteration of k-means is: 87,                the average loss is 102.07559681107672.\n",
      "The current iteration of k-means is: 88,                the average loss is 102.07032381666652.\n",
      "The current iteration of k-means is: 89,                the average loss is 102.06680034359265.\n",
      "The current iteration of k-means is: 90,                the average loss is 102.06400353811354.\n",
      "The current iteration of k-means is: 91,                the average loss is 102.0619762093164.\n",
      "The current iteration of k-means is: 92,                the average loss is 102.06048792352924.\n",
      "The current iteration of k-means is: 93,                the average loss is 102.05966338644654.\n",
      "For k= 7 :\n",
      "The current iteration of k-means is: 0,                the average loss is 209.3436.\n",
      "The current iteration of k-means is: 1,                the average loss is 110.66320349076125.\n",
      "The current iteration of k-means is: 2,                the average loss is 108.95197315927015.\n",
      "The current iteration of k-means is: 3,                the average loss is 107.58116843682721.\n",
      "The current iteration of k-means is: 4,                the average loss is 106.6947082535944.\n",
      "The current iteration of k-means is: 5,                the average loss is 106.02647544430306.\n",
      "The current iteration of k-means is: 6,                the average loss is 105.50642005995381.\n",
      "The current iteration of k-means is: 7,                the average loss is 105.15261869702512.\n",
      "The current iteration of k-means is: 8,                the average loss is 104.9682215299963.\n",
      "The current iteration of k-means is: 9,                the average loss is 104.83632659034797.\n",
      "The current iteration of k-means is: 10,                the average loss is 104.74288573482609.\n",
      "The current iteration of k-means is: 11,                the average loss is 104.66817606687368.\n",
      "The current iteration of k-means is: 12,                the average loss is 104.58314891628713.\n",
      "The current iteration of k-means is: 13,                the average loss is 104.48590941282835.\n",
      "The current iteration of k-means is: 14,                the average loss is 104.38392077584214.\n",
      "The current iteration of k-means is: 15,                the average loss is 104.2647997511976.\n",
      "The current iteration of k-means is: 16,                the average loss is 104.13427254719274.\n",
      "The current iteration of k-means is: 17,                the average loss is 103.98955459332271.\n",
      "The current iteration of k-means is: 18,                the average loss is 103.837283224551.\n",
      "The current iteration of k-means is: 19,                the average loss is 103.66776131519684.\n",
      "The current iteration of k-means is: 20,                the average loss is 103.50590873938184.\n",
      "The current iteration of k-means is: 21,                the average loss is 103.35228991611682.\n",
      "The current iteration of k-means is: 22,                the average loss is 103.22223585324362.\n",
      "The current iteration of k-means is: 23,                the average loss is 103.10488607826255.\n",
      "The current iteration of k-means is: 24,                the average loss is 103.02231657670008.\n",
      "The current iteration of k-means is: 25,                the average loss is 102.95793025988029.\n",
      "The current iteration of k-means is: 26,                the average loss is 102.9124490247024.\n",
      "The current iteration of k-means is: 27,                the average loss is 102.87224504227562.\n",
      "The current iteration of k-means is: 28,                the average loss is 102.83942395900371.\n",
      "The current iteration of k-means is: 29,                the average loss is 102.81124953770764.\n",
      "The current iteration of k-means is: 30,                the average loss is 102.78812461597605.\n",
      "The current iteration of k-means is: 31,                the average loss is 102.76168908410017.\n",
      "The current iteration of k-means is: 32,                the average loss is 102.73943900506218.\n",
      "The current iteration of k-means is: 33,                the average loss is 102.71853983207886.\n",
      "The current iteration of k-means is: 34,                the average loss is 102.69997777898138.\n",
      "The current iteration of k-means is: 35,                the average loss is 102.68262514976037.\n",
      "The current iteration of k-means is: 36,                the average loss is 102.66672638711974.\n",
      "The current iteration of k-means is: 37,                the average loss is 102.65175353921607.\n",
      "The current iteration of k-means is: 38,                the average loss is 102.63890933994448.\n",
      "The current iteration of k-means is: 39,                the average loss is 102.62674529613601.\n",
      "The current iteration of k-means is: 40,                the average loss is 102.61739289290843.\n",
      "The current iteration of k-means is: 41,                the average loss is 102.60740714438542.\n",
      "The current iteration of k-means is: 42,                the average loss is 102.59879604954536.\n",
      "The current iteration of k-means is: 43,                the average loss is 102.59151784842322.\n",
      "The current iteration of k-means is: 44,                the average loss is 102.5826559999645.\n",
      "The current iteration of k-means is: 45,                the average loss is 102.57553974898082.\n",
      "The current iteration of k-means is: 46,                the average loss is 102.56967737973717.\n",
      "The current iteration of k-means is: 47,                the average loss is 102.56528557285179.\n",
      "The current iteration of k-means is: 48,                the average loss is 102.56380058676397.\n",
      "The current iteration of k-means is: 49,                the average loss is 102.56263878925941.\n",
      "The current iteration of k-means is: 50,                the average loss is 102.56105555251474.\n",
      "The current iteration of k-means is: 51,                the average loss is 102.5602911847374.\n",
      "For k= 8 :\n",
      "The current iteration of k-means is: 0,                the average loss is 209.3436.\n",
      "The current iteration of k-means is: 1,                the average loss is 110.66320349076125.\n",
      "The current iteration of k-means is: 2,                the average loss is 108.95197315927015.\n",
      "The current iteration of k-means is: 3,                the average loss is 107.58116843682721.\n",
      "The current iteration of k-means is: 4,                the average loss is 106.6947082535944.\n",
      "The current iteration of k-means is: 5,                the average loss is 106.02647544430306.\n",
      "The current iteration of k-means is: 6,                the average loss is 105.50642005995381.\n",
      "The current iteration of k-means is: 7,                the average loss is 105.15060891920623.\n",
      "The current iteration of k-means is: 8,                the average loss is 104.88686558241078.\n",
      "The current iteration of k-means is: 9,                the average loss is 104.74224226533117.\n",
      "The current iteration of k-means is: 10,                the average loss is 104.6626202447702.\n",
      "The current iteration of k-means is: 11,                the average loss is 104.5855764922168.\n",
      "The current iteration of k-means is: 12,                the average loss is 104.52344804900775.\n",
      "The current iteration of k-means is: 13,                the average loss is 104.44560448034115.\n",
      "The current iteration of k-means is: 14,                the average loss is 104.34603852021039.\n",
      "The current iteration of k-means is: 15,                the average loss is 104.24417781863242.\n",
      "The current iteration of k-means is: 16,                the average loss is 104.1393701139682.\n",
      "The current iteration of k-means is: 17,                the average loss is 104.00740854436212.\n",
      "The current iteration of k-means is: 18,                the average loss is 103.85359922894642.\n",
      "The current iteration of k-means is: 19,                the average loss is 103.69365799600948.\n",
      "The current iteration of k-means is: 20,                the average loss is 103.52269377783159.\n",
      "The current iteration of k-means is: 21,                the average loss is 103.37470894635585.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current iteration of k-means is: 22,                the average loss is 103.24384012953261.\n",
      "The current iteration of k-means is: 23,                the average loss is 103.13024683670167.\n",
      "The current iteration of k-means is: 24,                the average loss is 103.02794745940601.\n",
      "The current iteration of k-means is: 25,                the average loss is 102.95128690489375.\n",
      "The current iteration of k-means is: 26,                the average loss is 102.88974517914396.\n",
      "The current iteration of k-means is: 27,                the average loss is 102.83900602666742.\n",
      "The current iteration of k-means is: 28,                the average loss is 102.79609874677591.\n",
      "The current iteration of k-means is: 29,                the average loss is 102.76072206928269.\n",
      "The current iteration of k-means is: 30,                the average loss is 102.72942422022011.\n",
      "The current iteration of k-means is: 31,                the average loss is 102.70729330922958.\n",
      "The current iteration of k-means is: 32,                the average loss is 102.68831655116949.\n",
      "The current iteration of k-means is: 33,                the average loss is 102.66874184821033.\n",
      "The current iteration of k-means is: 34,                the average loss is 102.6513226484357.\n",
      "The current iteration of k-means is: 35,                the average loss is 102.63664273631737.\n",
      "The current iteration of k-means is: 36,                the average loss is 102.61853561620512.\n",
      "The current iteration of k-means is: 37,                the average loss is 102.60347038511655.\n",
      "The current iteration of k-means is: 38,                the average loss is 102.58732881079793.\n",
      "The current iteration of k-means is: 39,                the average loss is 102.57291078834555.\n",
      "The current iteration of k-means is: 40,                the average loss is 102.55752549553891.\n",
      "The current iteration of k-means is: 41,                the average loss is 102.54368250813347.\n",
      "The current iteration of k-means is: 42,                the average loss is 102.53451019498098.\n",
      "The current iteration of k-means is: 43,                the average loss is 102.5261895202699.\n",
      "The current iteration of k-means is: 44,                the average loss is 102.51672174049526.\n",
      "The current iteration of k-means is: 45,                the average loss is 102.50561691402099.\n",
      "The current iteration of k-means is: 46,                the average loss is 102.49447092794823.\n",
      "The current iteration of k-means is: 47,                the average loss is 102.48890965428399.\n",
      "The current iteration of k-means is: 48,                the average loss is 102.48267574481905.\n",
      "The current iteration of k-means is: 49,                the average loss is 102.47853063857787.\n",
      "The current iteration of k-means is: 50,                the average loss is 102.47526453500765.\n",
      "The current iteration of k-means is: 51,                the average loss is 102.4730207266662.\n",
      "The current iteration of k-means is: 52,                the average loss is 102.47073340969578.\n",
      "The current iteration of k-means is: 53,                the average loss is 102.46984473513525.\n",
      "For k= 9 :\n",
      "The current iteration of k-means is: 0,                the average loss is 209.3436.\n",
      "The current iteration of k-means is: 1,                the average loss is 110.66320349076125.\n",
      "The current iteration of k-means is: 2,                the average loss is 108.95197315927015.\n",
      "The current iteration of k-means is: 3,                the average loss is 107.58116843682721.\n",
      "The current iteration of k-means is: 4,                the average loss is 106.6947082535944.\n",
      "The current iteration of k-means is: 5,                the average loss is 106.02647544430306.\n",
      "The current iteration of k-means is: 6,                the average loss is 105.50642005995381.\n",
      "The current iteration of k-means is: 7,                the average loss is 105.15060891920623.\n",
      "The current iteration of k-means is: 8,                the average loss is 104.8851730897352.\n",
      "The current iteration of k-means is: 9,                the average loss is 104.63161225488365.\n",
      "The current iteration of k-means is: 10,                the average loss is 104.5468013825139.\n",
      "The current iteration of k-means is: 11,                the average loss is 104.46245484895502.\n",
      "The current iteration of k-means is: 12,                the average loss is 104.38143499137048.\n",
      "The current iteration of k-means is: 13,                the average loss is 104.3208305107775.\n",
      "The current iteration of k-means is: 14,                the average loss is 104.24902355031954.\n",
      "The current iteration of k-means is: 15,                the average loss is 104.16747411238028.\n",
      "The current iteration of k-means is: 16,                the average loss is 104.07697019575457.\n",
      "The current iteration of k-means is: 17,                the average loss is 103.96100137873754.\n",
      "The current iteration of k-means is: 18,                the average loss is 103.8294871686497.\n",
      "The current iteration of k-means is: 19,                the average loss is 103.66461058962017.\n",
      "The current iteration of k-means is: 20,                the average loss is 103.49804787035704.\n",
      "The current iteration of k-means is: 21,                the average loss is 103.31647227158616.\n",
      "The current iteration of k-means is: 22,                the average loss is 103.17666903825321.\n",
      "The current iteration of k-means is: 23,                the average loss is 103.04702788056348.\n",
      "The current iteration of k-means is: 24,                the average loss is 102.94468152164956.\n",
      "The current iteration of k-means is: 25,                the average loss is 102.86290394451082.\n",
      "The current iteration of k-means is: 26,                the average loss is 102.79279064697725.\n",
      "The current iteration of k-means is: 27,                the average loss is 102.73576385207738.\n",
      "The current iteration of k-means is: 28,                the average loss is 102.67932057214485.\n",
      "The current iteration of k-means is: 29,                the average loss is 102.63560940224386.\n",
      "The current iteration of k-means is: 30,                the average loss is 102.59922683469175.\n",
      "The current iteration of k-means is: 31,                the average loss is 102.57004618518414.\n",
      "The current iteration of k-means is: 32,                the average loss is 102.54481842022673.\n",
      "The current iteration of k-means is: 33,                the average loss is 102.52786908181932.\n",
      "The current iteration of k-means is: 34,                the average loss is 102.51121323487708.\n",
      "The current iteration of k-means is: 35,                the average loss is 102.49995667812827.\n",
      "The current iteration of k-means is: 36,                the average loss is 102.4902199247573.\n",
      "The current iteration of k-means is: 37,                the average loss is 102.48048041640992.\n",
      "The current iteration of k-means is: 38,                the average loss is 102.47379475303298.\n",
      "The current iteration of k-means is: 39,                the average loss is 102.46647820079991.\n",
      "The current iteration of k-means is: 40,                the average loss is 102.46111772900053.\n",
      "The current iteration of k-means is: 41,                the average loss is 102.45527467475974.\n",
      "The current iteration of k-means is: 42,                the average loss is 102.4500552070917.\n",
      "The current iteration of k-means is: 43,                the average loss is 102.4458438935836.\n",
      "The current iteration of k-means is: 44,                the average loss is 102.44119954329145.\n",
      "The current iteration of k-means is: 45,                the average loss is 102.4367711964679.\n",
      "The current iteration of k-means is: 46,                the average loss is 102.43228973171401.\n",
      "The current iteration of k-means is: 47,                the average loss is 102.42791687226067.\n",
      "The current iteration of k-means is: 48,                the average loss is 102.42414883464552.\n",
      "The current iteration of k-means is: 49,                the average loss is 102.42028944824926.\n",
      "The current iteration of k-means is: 50,                the average loss is 102.41515224379154.\n",
      "The current iteration of k-means is: 51,                the average loss is 102.40886901451574.\n",
      "The current iteration of k-means is: 52,                the average loss is 102.40528955177828.\n",
      "The current iteration of k-means is: 53,                the average loss is 102.40100196712557.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current iteration of k-means is: 54,                the average loss is 102.39846059283292.\n",
      "The current iteration of k-means is: 55,                the average loss is 102.39718994686177.\n",
      "The current iteration of k-means is: 56,                the average loss is 102.39669981194423.\n",
      "For k= 10 :\n",
      "The current iteration of k-means is: 0,                the average loss is 209.3436.\n",
      "The current iteration of k-means is: 1,                the average loss is 110.66320349076125.\n",
      "The current iteration of k-means is: 2,                the average loss is 108.95197315927015.\n",
      "The current iteration of k-means is: 3,                the average loss is 107.58116843682721.\n",
      "The current iteration of k-means is: 4,                the average loss is 106.6947082535944.\n",
      "The current iteration of k-means is: 5,                the average loss is 106.02647544430306.\n",
      "The current iteration of k-means is: 6,                the average loss is 105.50642005995381.\n",
      "The current iteration of k-means is: 7,                the average loss is 105.15060891920623.\n",
      "The current iteration of k-means is: 8,                the average loss is 104.8851730897352.\n",
      "The current iteration of k-means is: 9,                the average loss is 104.63151865796439.\n",
      "The current iteration of k-means is: 10,                the average loss is 104.52814516056438.\n",
      "The current iteration of k-means is: 11,                the average loss is 104.41145996689792.\n",
      "The current iteration of k-means is: 12,                the average loss is 104.29454286267958.\n",
      "The current iteration of k-means is: 13,                the average loss is 104.22928107542647.\n",
      "The current iteration of k-means is: 14,                the average loss is 104.15382477830232.\n",
      "The current iteration of k-means is: 15,                the average loss is 104.05639923331144.\n",
      "The current iteration of k-means is: 16,                the average loss is 103.94317596128444.\n",
      "The current iteration of k-means is: 17,                the average loss is 103.8267525160457.\n",
      "The current iteration of k-means is: 18,                the average loss is 103.68651658419253.\n",
      "The current iteration of k-means is: 19,                the average loss is 103.51453905412583.\n",
      "The current iteration of k-means is: 20,                the average loss is 103.33978861588757.\n",
      "The current iteration of k-means is: 21,                the average loss is 103.16228460868676.\n",
      "The current iteration of k-means is: 22,                the average loss is 103.01307675010626.\n",
      "The current iteration of k-means is: 23,                the average loss is 102.88753507988443.\n",
      "The current iteration of k-means is: 24,                the average loss is 102.79315906725422.\n",
      "The current iteration of k-means is: 25,                the average loss is 102.70897042086713.\n",
      "The current iteration of k-means is: 26,                the average loss is 102.64450234579945.\n",
      "The current iteration of k-means is: 27,                the average loss is 102.58571019095145.\n",
      "The current iteration of k-means is: 28,                the average loss is 102.54126190473221.\n",
      "The current iteration of k-means is: 29,                the average loss is 102.50512948097497.\n",
      "The current iteration of k-means is: 30,                the average loss is 102.47146848715933.\n",
      "The current iteration of k-means is: 31,                the average loss is 102.44268497703041.\n",
      "The current iteration of k-means is: 32,                the average loss is 102.4238727526553.\n",
      "The current iteration of k-means is: 33,                the average loss is 102.40730210013233.\n",
      "The current iteration of k-means is: 34,                the average loss is 102.38889667226536.\n",
      "The current iteration of k-means is: 35,                the average loss is 102.37157502172093.\n",
      "The current iteration of k-means is: 36,                the average loss is 102.35687477217724.\n",
      "The current iteration of k-means is: 37,                the average loss is 102.33981786792937.\n",
      "The current iteration of k-means is: 38,                the average loss is 102.32793326514398.\n",
      "The current iteration of k-means is: 39,                the average loss is 102.31754098412138.\n",
      "The current iteration of k-means is: 40,                the average loss is 102.30826884181313.\n",
      "The current iteration of k-means is: 41,                the average loss is 102.3015147781488.\n",
      "The current iteration of k-means is: 42,                the average loss is 102.29399104390063.\n",
      "The current iteration of k-means is: 43,                the average loss is 102.28928660549289.\n",
      "The current iteration of k-means is: 44,                the average loss is 102.285152913171.\n",
      "The current iteration of k-means is: 45,                the average loss is 102.28168808077449.\n",
      "The current iteration of k-means is: 46,                the average loss is 102.2784909597975.\n",
      "The current iteration of k-means is: 47,                the average loss is 102.27670066325996.\n",
      "The current iteration of k-means is: 48,                the average loss is 102.27371505670855.\n",
      "The current iteration of k-means is: 49,                the average loss is 102.27224246202537.\n",
      "The current iteration of k-means is: 50,                the average loss is 102.26938420431604.\n",
      "The current iteration of k-means is: 51,                the average loss is 102.2683480265575.\n",
      "The current iteration of k-means is: 52,                the average loss is 102.26721335310181.\n",
      "The current iteration of k-means is: 53,                the average loss is 102.26578309494228.\n",
      "The current iteration of k-means is: 54,                the average loss is 102.26453970646978.\n",
      "The current iteration of k-means is: 55,                the average loss is 102.26341099830735.\n",
      "The current iteration of k-means is: 56,                the average loss is 102.26272553827222.\n",
      "The best loss is: 1.046910423833394  and the best k is: 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Defining the aggregation method\n",
    "agg=cluster_agg\n",
    "#Max number of clusters\n",
    "k = 10\n",
    "max_iters = 100\n",
    "threshold = 1e-12\n",
    "#Initializing the array of test losses\n",
    "losses=[]\n",
    "#Grid-searching the number of clusters by cross-validation\n",
    "for i in range(1,k+1):\n",
    "    print('For k=',i,':')\n",
    "    #Training the model\n",
    "    assignments, mu, _=kmeans(train, i, max_iters, threshold)\n",
    "    #Computing the test RMSE\n",
    "    loss=np.nanmean((test-agg(assignments, mu, i, train.to_numpy()))**2)**(1/2)\n",
    "    #Adding the test RMSE to the array of losses\n",
    "    losses.append(loss)\n",
    "#Plotting 'RMSE vs k'\n",
    "plt.plot(range(1,k+1),losses)\n",
    "#Getting the best loss and k\n",
    "k_opt=np.argmin(losses)+1\n",
    "min_loss=np.min(losses)\n",
    "print('The best loss is:',min_loss,' and the best k is:',k_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current iteration of k-means is: 0,                the average loss is 351.2401170958377.\n",
      "The current iteration of k-means is: 1,                the average loss is 121.10913726420793.\n",
      "The current iteration of k-means is: 2,                the average loss is 116.73341462442188.\n",
      "The current iteration of k-means is: 3,                the average loss is 115.49334925559329.\n",
      "The current iteration of k-means is: 4,                the average loss is 114.96369887321319.\n"
     ]
    }
   ],
   "source": [
    "#Defining the remaining model parameters\n",
    "max_iters = 100\n",
    "threshold = 1\n",
    "#Building the model on the full dataset\n",
    "assignments, mu, _=kmeans(cli_vs_mov, k_opt, max_iters, threshold)\n",
    "#Generation the predictions\n",
    "classified=cluster_agg(assignments, mu,k_opt,cli_vs_mov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Row</th>\n",
       "      <th>Col</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Row Col  Rating\n",
       "0      1   1     3.0\n",
       "1     10   1     3.0\n",
       "2    100   1     3.0\n",
       "3   1000   1     4.0\n",
       "4  10000   1     4.0"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Converting the columns back to the Col column and making Row a column instead of an index\n",
    "classified=pd.melt(classified.reset_index(), id_vars=['Row'], var_name='Col', value_name='Rating')\n",
    "classified.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>r1_c1</th>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r10_c1</th>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r100_c1</th>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r1000_c1</th>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r10000_c1</th>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Rating\n",
       "r1_c1         3.0\n",
       "r10_c1        3.0\n",
       "r100_c1       3.0\n",
       "r1000_c1      4.0\n",
       "r10000_c1     4.0"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Converting Row and Col values into an id\n",
    "classified.index='r'+classified['Row']+'_c'+classified['Col']\n",
    "classified=classified.drop(columns=['Row','Col'])\n",
    "classified.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>r37_c1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>r73_c1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>r156_c1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>r160_c1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>r248_c1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id  Prediction\n",
       "0   r37_c1           3\n",
       "1   r73_c1           3\n",
       "2  r156_c1           3\n",
       "3  r160_c1           3\n",
       "4  r248_c1           3"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading the sample submission data\n",
    "sample_sumbission=pd.read_csv('../data/sampleSubmission.csv')\n",
    "sample_sumbission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>r1000_c1</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>r1141_c1</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>r1146_c1</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>r1157_c1</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>r1184_c1</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Id  Prediction\n",
       "0  r1000_c1         4.0\n",
       "1  r1141_c1         4.0\n",
       "2  r1146_c1         4.0\n",
       "3  r1157_c1         3.0\n",
       "4  r1184_c1         4.0"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classified=classified[classified.index.isin(list(sample_sumbission['Id']))].reset_index().rename(columns={'index': 'Id', 'Rating':'Prediction'})\n",
    "classified.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exporting the predictions\n",
    "classified.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
