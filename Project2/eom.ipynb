{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "#importing the models\n",
    "import Kmeans\n",
    "import ALS\n",
    "import NN\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def median_vote(predictions):\n",
    "    #computing the median\n",
    "    median=np.median(predictions,axis=0)\n",
    "    #making sure the result is an int (not the case if the amount of predictions is even)\n",
    "    bounded_median=np.floor(median)\n",
    "    return bounded_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mode_vote(predictions):\n",
    "    #computing the mode\n",
    "    return stats.mode(predictions,axis=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maj_vote(predictions):\n",
    "    top=predictions.iloc[0][:,np.newaxis]\n",
    "    mode=stats.mode(predictions,axis=0)\n",
    "    freq=mode[1].T/predictions.shape[0]\n",
    "    return np.where(freq<0.5,top,mode[0].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_vote(predictions):\n",
    "    return np.round(np.mean(predictions,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vote(voting_f):\n",
    "    #useful constants\n",
    "    submission_path='submission.csv'\n",
    "    training_path = \"data/data_train.csv\"\n",
    "    format_path = \"data/sampleSubmission.csv\"\n",
    "    #Loading the data\n",
    "    print(\"Loading datasets\")\n",
    "    try:\n",
    "        input_ = pd.read_csv(training_path)\n",
    "        format_ = pd.read_csv(format_path)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Impossible to load training or format files, \"\n",
    "              \"please double check\")\n",
    "        return pd.DataFrame([])\n",
    "    #computing the prediction of the ALS algorithm\n",
    "    predictions=ALS.main(input_.copy(), format_.copy())\n",
    "    print(predictions.head(),format_.head())\n",
    "    #computing multiple predictions of the kmeans algorithm\n",
    "    for k in [6]:\n",
    "        predictions=predictions.merge(Kmeans.main(input_.copy(), format_.copy(), k),on='Id')\n",
    "        print(predictions.shape)\n",
    "    #computing the prediction of the NN algorithm\n",
    "    predictions=predictions.merge(NN.main(input_.copy(), format_.copy()),on='Id')\n",
    "    print(predictions.shape)\n",
    "    #setting 'Id' as the index of the aggregation of predictions\n",
    "    predictions.set_index('Id', inplace=True)\n",
    "    #finding the best prediction through the voting function\n",
    "    print('Voting...')\n",
    "    predictions['Prediction']=voting_f(predictions.T)\n",
    "    #exporting the final prediction using the submission path\n",
    "    print('Exporting the final prediction...')\n",
    "    predictions[['Prediction']].to_csv(submission_path)\n",
    "    print('Done!')\n",
    "    return predictions[['Prediction']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets\n",
      "Spliting train/test\n",
      "the shape of original ratings. (# of row, # of col): (10000, 1000)\n",
      "Splitting progression: 100.0%\n",
      "Total number of nonzero elements in original data:1,176,952\n",
      "Total number of nonzero elements in train data:1,065,169\n",
      "Total number of nonzero elements in test data:111,783\n",
      "Trying to retrieve cached optimal matrix factorization\n",
      "Successfully retrieved cached optimal matrix factorization\n",
      "Emitting predictions 1176952/1176952\n",
      "       Id  Prediction\n",
      "0   r1_c6           5\n",
      "1  r1_c33           3\n",
      "2  r1_c41           3\n",
      "3  r1_c65           2\n",
      "4  r1_c71           3         Id  Prediction\n",
      "0   r37_c1           3\n",
      "1   r73_c1           3\n",
      "2  r156_c1           3\n",
      "3  r160_c1           3\n",
      "4  r248_c1           3\n",
      "Kmeans for k= 6 :\n",
      "The current iteration of k-means is: 24, the average loss is 113.83491094376913.\n",
      "(146635, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajlre\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1059256 samples, validate on 117696 samples\n",
      "Epoch 1/10\n",
      "1059256/1059256 [==============================] - 30s 28us/step - loss: 1.0349 - val_loss: 1.0140\n",
      "Epoch 2/10\n",
      "1059256/1059256 [==============================] - 30s 28us/step - loss: 1.0071 - val_loss: 1.0080\n",
      "Epoch 3/10\n",
      "1059256/1059256 [==============================] - 30s 28us/step - loss: 0.9999 - val_loss: 1.0021\n",
      "Epoch 4/10\n",
      "1059256/1059256 [==============================] - 32s 30us/step - loss: 0.9950 - val_loss: 1.0014\n",
      "Epoch 5/10\n",
      "1059256/1059256 [==============================] - 33s 31us/step - loss: 0.9918 - val_loss: 0.9990\n",
      "Epoch 6/10\n",
      "1059256/1059256 [==============================] - 34s 32us/step - loss: 0.9894 - val_loss: 1.0021\n",
      "Epoch 7/10\n",
      "1059256/1059256 [==============================] - 33s 31us/step - loss: 0.9875 - val_loss: 1.0001\n",
      "Epoch 8/10\n",
      "1059256/1059256 [==============================] - 32s 30us/step - loss: 0.9859 - val_loss: 0.9986\n",
      "Epoch 9/10\n",
      "1059256/1059256 [==============================] - 31s 30us/step - loss: 0.9844 - val_loss: 1.0008\n",
      "Epoch 10/10\n",
      "1059256/1059256 [==============================] - 31s 30us/step - loss: 0.9830 - val_loss: 1.0062\n",
      "Generating predictions\n",
      "(146635, 4)\n",
      "Voting...\n",
      "Exporting the final prediction...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "coco=vote(cluster_vote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#useful constants\n",
    "submission_path='submission.csv'\n",
    "training_path = \"data/data_train.csv\"\n",
    "format_path = \"data/sampleSubmission.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the data\n",
    "print(\"Loading datasets\")\n",
    "try:\n",
    "    input_ = pd.read_csv(training_path)\n",
    "    format_ = pd.read_csv(format_path)\n",
    "except FileNotFoundError:\n",
    "    print(\"Impossible to load training or format files, \"\n",
    "          \"please double check\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data\n",
    "train, test =sklearn.model_selection.train_test_split(input_,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing the predictions of the ALS algorithm\n",
    "predictions_als=ALS.main(train.copy(), test.copy()).set_index(\"Id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing the best prediction of the kmeans algorithm\n",
    "k=6\n",
    "predictions_kmeans = Kmeans.main(train.copy(), test.copy(), k).set_index(\"Id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing the prediction of the NN algorithm\n",
    "predictions_nn = NN.main(train.copy(), test.copy()).set_index(\"Id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#putting all the predictions in a same table\n",
    "concat = pd.merge(pd.merge(predictions_kmeans, predictions_als, left_index=True, right_index=True), predictions_nn, left_index=True, right_index=True)\n",
    "concat.columns = [\"Pred1\", \"Pred2\", \"Pred3\"]\n",
    "#approximating the rmse through cross-validation\n",
    "voting_f=cluster_vote\n",
    "concat['Prediction']=voting_f(concat[[\"Pred1\", \"Pred2\", \"Pred3\"]].T,[w1,w2,w3])\n",
    "print(np.mean((test.set_index(\"Id\")-concat[['Prediction']])**2)**(1/2))\n",
    "concat.drop([\"Pred1\", \"Pred2\", \"Pred3\"], axis=1, inplace=True)\n",
    "concat.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
