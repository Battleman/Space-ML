# CS-433: Project 2 - Recommender System
This directory contains the code and results for the second project of CS-433 (Machine Learning) at EPFL.
In particular, the project we selected is the Recommender System.

## Goal
The goal of this project was to train a recommender system. This means, we get (very sparse) ratings of 10'000 users, for 1'000 items (presumably, films).
We tried to elaborate a model that allows predicting what a user would rate an item that they haven't rated yet.

This allows for any platform that suggest content based on ratings (implicit and/or explicit) to find items to recommend to its users.

## Installation
The main dependencies for our project are the following:
* Keras
* Numpy
* Pandas
* Scikit-learn
* Scikit-surprise
* Tensorflow
* Pickle

You can either install those yourself, or use the provided requirements.txt. The file was generated by anaconda, meaning you can simple run
`conda create -n <name of env> --file requirements.txt`
## Running
Simple! Just place the required CSV files in the `data/` folder, and run the main file: `python run.py`.
### Required files
The program relies on 2 input files:
* `data/data_train.csv` is a CSV file containing the known ratings
* `data/sampleSubmission.csv` is a CSV file containing the IDs to predict, with dummy ratings (you can put whatever)

#### Files format
The aforementioned CSV files must have a very specific format:
* First line is the title of columns (“ID” and “Prediction”)
* Each of the following lines contain respectively the id of the rating and the rating.
  * The rating is an integer in the 1-5 range (included).
  * The indicates the id of the user, and id of the item as follows: `rX_cY` with X = id of the user (unique integer) and `Y` is the ID of the item (again, an integer). Imagine them as their position (row-column) in a big matrix.
### Resources
Mind that running this program takes some resources. Training from ground 0 takes about 9-10 hours on a desktop computer, and around 30G of memory (or SWAP, if you are brave). Half can be avoided by keeping the `ridge_coefs.pkl` in the root of the folders, that specifies the coefficients of a ridge regression. Furthermore, once you've run once, a lot of computations are cached, allowing you to down the runtime to a few minutes.

## Directory structures

```
.
├── ALS/         Files of the ALS model
│   └── cache/   Placeholder to contain all cached for ALS
├── data/        Where to put the required CSV
├── figures/     Some plots; you can ignore this
├── Kmeans/      Files of to the k-Means model
├── NN/          Files Neural Networks model
└── Surprize/    Files of the other models, from the Surprise library.
    └── cache/   Placeholder to contain all cached for Surprise
```

Some interesting files:
```
├── README.md           This README
├── requirements.txt    Python requirements
├── ridge_coefs.pkl     Cached coefficients for the AiCrowd challenge
├── run.py              File to run
├── submission.csv      Output of the predictions
